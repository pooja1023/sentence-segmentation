{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text segmentation - Wikipedia\n",
      "Text segmentation\n",
      "From Wikipedia, the free encyclopedia\n",
      "Jump to:\t\t\t\t\tnavigation, \t\t\t\t\tsearch\n",
      "This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (October 2011) (Learn how and when to remove this template message)\n",
      "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics. The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing. The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.\n",
      "Compare speech segmentation, the process of dividing speech into linguistically meaningful portions.\n",
      "Contents\n",
      "1 Segmentation problems\n",
      "1.1 Word segmentation\n",
      "1.2 Intent segmentation\n",
      "1.3 Sentence segmentation\n",
      "1.4 Topic segmentation\n",
      "1.5 Other segmentation problems\n",
      "2 Automatic segmentation approaches\n",
      "3 See also\n",
      "4 References\n",
      "5 External links\n",
      "Segmentation problems[edit]\n",
      "Word segmentation[edit]\n",
      "See also: Word § Word boundaries\n",
      "Word segmentation is the problem of dividing a string of written language into its component words.\n",
      "In English and many other languages using some form of the Latin alphabet, the space is a good approximation of a word divider (word delimiter), although this concept has limits because of the variability with which languages emically regard collocations and compounds. Many English compound nouns are variably written (for example, ice box = ice-box = icebox; pig sty = pig-sty = pigsty) with a corresponding variation in whether speakers think of them as noun phrases or single nouns; there are trends in how norms are set, such as that open compounds often tend eventually to solidify by widespread convention, but variation remains systemic. In contrast, German compound nouns show less orthographic variation, with solidification being a stronger norm.\n",
      "However, the equivalent to the word space character is not found in all written scripts, and without it word segmentation is a difficult problem. Languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited.\n",
      "In some writing systems however, such as the Ge'ez script used for Amharic and Tigrinya among other languages, words are explicitly delimited (at least historically) with a non-whitespace character.\n",
      "The Unicode Consortium has published a Standard Annex on Text Segmentation,[1] exploring the issues of segmentation in multiscript texts.\n",
      "Word splitting is the process of parsing concatenated text (i.e. text that contains no spaces or other word separators) to infer where word breaks exist.\n",
      "Word splitting may also refer to the process of hyphenation.\n",
      "Intent segmentation[edit]\n",
      "See also: Tri-box method\n",
      "Intent segmentation is the problem of dividing written words into keyphrases (2 or more group of words).\n",
      "In English and all other languages the core intent or desire is identified and become the corner-stone of the keyphrase Intent segmentation. Core product/service, idea, action & or thought anchor the keyphrase.\n",
      "\"[All things are made of atoms]. [Little particles that move] [around in perpetual motion], [attraction each other] [when they are a little distance apart], [but repelling] [upon being squeezed] [into one another].\"\n",
      "Sentence segmentation[edit]\n",
      "See also: Sentence boundary disambiguation\n",
      "Sentence segmentation is the problem of dividing a string of written language into its component sentences. In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation. However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence. For example Mr. is not its own sentence in \"Mr. Smith went to the shops in Jones Street.\" When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries.\n",
      "As with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries.\n",
      "Topic segmentation[edit]\n",
      "Main articles: Topic analysis and Document classification\n",
      "Topic analysis consists of two main tasks: topic identiﬁcation and text segmentation. While the first is a simple classification of a specific text, the latter case implies that a document may contain multiple topics, and the task of computerized text segmentation may be to discover these topics automatically and segment the text accordingly. The topic boundaries may be apparent from section titles and paragraphs. In other cases, one needs to use techniques similar to those used in document classification.\n",
      "Segmenting the text into topics or discourse turns might be useful in some natural processing tasks: it can improve information retrieval or speech recognition significantly (by indexing/recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result). It is also needed in topic detection and tracking systems and text summarizing problems.\n",
      "Many different approaches have been tried:[2][3] e.g. HMM, lexical chains, passage similarity using word co-occurrence, clustering, topic modeling, etc.\n",
      "It is quite an ambiguous task – people evaluating the text segmentation systems often differ in topic boundaries. Hence, text segment evaluation is also a challenging problem.\n",
      "Other segmentation problems[edit]\n",
      "Processes may be required to segment text into segments besides mentioned, including morphemes (a task usually called morphological analysis) or paragraphs.\n",
      "Automatic segmentation approaches[edit]\n",
      "Automatic segmentation is the problem in natural language processing of implementing a computer process to segment text.\n",
      "When punctuation and similar clues are not consistently available, the segmentation task often requires fairly non-trivial techniques, such as statistical decision-making, large dictionaries, as well as consideration of syntactic and semantic constraints. Effective natural language processing systems and text segmentation tools usually operate on text in specific domains and sources. As an example, processing text used in medical records is a very different problem than processing news articles or real estate advertisements.\n",
      "The process of developing text segmentation tools starts with collecting a large corpus of text in an application domain. There are two general approaches:\n",
      "Manual analysis of text and writing custom software\n",
      "Annotate the sample corpus with boundary information and use machine learning\n",
      "Some text segmentation systems take advantage of any markup like HTML and know document formats like PDF to provide additional evidence for sentence and paragraph boundaries.\n",
      "See also[edit]\n",
      "Hyphenation\n",
      "Natural language processing\n",
      "Speech segmentation\n",
      "Lexical analysis\n",
      "Word count\n",
      "Line breaking\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "General terms\n",
      "Text corpus\n",
      "Speech corpus\n",
      "Stopwords\n",
      "Bag-of-words\n",
      "AI-complete\n",
      "n-gram (Bigram, Trigram)\n",
      "Text analysis\n",
      "Text segmentation\n",
      "Part-of-speech tagging\n",
      "Text chunking\n",
      "Compound term processing\n",
      "Collocation extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "Named-entity recognition\n",
      "Coreference resolution\n",
      "Sentiment analysis\n",
      "Concept mining\n",
      "Parsing\n",
      "Word-sense disambiguation\n",
      "Terminology extraction\n",
      "Truecasing\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Automatic identification\n",
      "and data capture\n",
      "Speech recognition\n",
      "Speech synthesis\n",
      "Optical character recognition\n",
      "Natural language generation\n",
      "Topic model\n",
      "Pachinko allocation\n",
      "Latent Dirichlet allocation\n",
      "Latent semantic analysis\n",
      "Computer-assisted\n",
      "reviewing\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Spell checker\n",
      "Syntax guessing\n",
      "Natural language\n",
      "user interface\n",
      "Automated online assistant\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Question answering\n",
      "References[edit]\n",
      "^ UAX #29\n",
      "^ Freddy Y. Y. Choi (2000). \"Advances in domain independent linear text segmentation\" (PDF). Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL-00). pp. 26–33.\n",
      "^ Jeffrey C. Reynar (1998). \"Topic Segmentation: Algorithms and Applications\" (PDF). IRCS-98-21. University of Pennsylvania. Retrieved 2007-11-08.\n",
      "External links[edit]\n",
      "Word Segment An open source software tool for word segmentation in Chinese.\n",
      "Word Split An open source software tool designed to split conjoined words into human-readable text.\n",
      "Stanford Segmenter An open source software tool for word segmentation in Chinese or morpheme segmentation in Arabic.\n",
      "KyTea An open source software tool for word segmentation in Japanese and Chinese.\n",
      "Chinese Notes A Chinese–English dictionary that also does word segmentation.\n",
      "Zhihuita Segmentor A high precision and high performance Chinese segmentation freeware.\n",
      "Python wordsegment module An open source Python module for English word segmentation.\n",
      "Retrieved from \"https://en.wikipedia.org/w/index.php?title=Text_segmentation&oldid=792337667\"\n",
      "Categories: Tasks of natural language processingHidden categories: Use dmy dates from March 2016Articles needing additional references from October 2011All articles needing additional references\n",
      "Navigation menu\n",
      "Personal tools\n",
      "Not logged inTalkContributionsCreate accountLog in\n",
      "Namespaces\n",
      "Article\n",
      "Talk\n",
      "Variants\n",
      "Views\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "More\n",
      "Search\n",
      "Navigation\n",
      "Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store\n",
      "Interaction\n",
      "HelpAbout WikipediaCommunity portalRecent changesContact page\n",
      "Tools\n",
      "What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page\n",
      "Print/export\n",
      "Create a bookDownload as PDFPrintable version\n",
      "Languages\n",
      "বাংলাDeutschGalegoՀայերենBahasa Indonesia日本語Norsk nynorskPortuguêsРусский\n",
      "Edit links\n",
      "This page was last edited on 25 July 2017, at 22:51.\n",
      "Text is available under the Creative Commons Attribution-ShareAlike License;\n",
      "additional terms may apply.\n",
      "By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Developers\n",
      "Cookie statement\n",
      "Mobile view\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\anaconda\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Text_segmentation#Sentence_segmentation\"\n",
    "html = urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "    script.extract()    \n",
    "\n",
    "text = soup.get_text()\n",
    "\n",
    "\n",
    "lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SENTENCE TOKENIZATION\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize_list = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text segmentation - Wikipedia\\nText segmentation\\nFrom Wikipedia, the free encyclopedia\\nJump to:\\t\\t\\t\\t\\tnavigation, \\t\\t\\t\\t\\tsearch\\nThis article needs additional citations for verification.',\n",
       " 'Please help improve this article by adding citations to reliable sources.',\n",
       " 'Unsourced material may be challenged and removed.',\n",
       " '(October 2011) (Learn how and when to remove this template message)\\nText segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics.',\n",
       " 'The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing.',\n",
       " 'The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.',\n",
       " 'Compare speech segmentation, the process of dividing speech into linguistically meaningful portions.',\n",
       " 'Contents\\n1 Segmentation problems\\n1.1 Word segmentation\\n1.2 Intent segmentation\\n1.3 Sentence segmentation\\n1.4 Topic segmentation\\n1.5 Other segmentation problems\\n2 Automatic segmentation approaches\\n3 See also\\n4 References\\n5 External links\\nSegmentation problems[edit]\\nWord segmentation[edit]\\nSee also: Word §\\xa0Word boundaries\\nWord segmentation is the problem of dividing a string of written language into its component words.',\n",
       " 'In English and many other languages using some form of the Latin alphabet, the space is a good approximation of a word divider (word delimiter), although this concept has limits because of the variability with which languages emically regard collocations and compounds.',\n",
       " 'Many English compound nouns are variably written (for example, ice\\xa0box = ice-box = icebox; pig\\xa0sty = pig-sty = pigsty) with a corresponding variation in whether speakers think of them as noun phrases or single nouns; there are trends in how norms are set, such as that open compounds often tend eventually to solidify by widespread convention, but variation remains systemic.',\n",
       " 'In contrast, German compound nouns show less orthographic variation, with solidification being a stronger norm.',\n",
       " 'However, the equivalent to the word space character is not found in all written scripts, and without it word segmentation is a difficult problem.',\n",
       " 'Languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited.',\n",
       " \"In some writing systems however, such as the Ge'ez script used for Amharic and Tigrinya among other languages, words are explicitly delimited (at least historically) with a non-whitespace character.\",\n",
       " 'The Unicode Consortium has published a Standard Annex on Text Segmentation,[1] exploring the issues of segmentation in multiscript texts.',\n",
       " 'Word splitting is the process of parsing concatenated text (i.e.',\n",
       " 'text that contains no spaces or other word separators) to infer where word breaks exist.',\n",
       " 'Word splitting may also refer to the process of hyphenation.',\n",
       " 'Intent segmentation[edit]\\nSee also: Tri-box method\\nIntent segmentation is the problem of dividing written words into keyphrases (2 or more group of words).',\n",
       " 'In English and all other languages the core intent or desire is identified and become the corner-stone of the keyphrase Intent segmentation.',\n",
       " 'Core product/service, idea, action & or thought anchor the keyphrase.',\n",
       " '\"[All things are made of atoms].',\n",
       " '[Little particles that move] [around in perpetual motion], [attraction each other] [when they are a little distance apart], [but repelling] [upon being squeezed] [into one another].\"',\n",
       " 'Sentence segmentation[edit]\\nSee also: Sentence boundary disambiguation\\nSentence segmentation is the problem of dividing a string of written language into its component sentences.',\n",
       " 'In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation.',\n",
       " 'However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence.',\n",
       " 'For example Mr. is not its own sentence in \"Mr. Smith went to the shops in Jones Street.\"',\n",
       " 'When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries.',\n",
       " 'As with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries.',\n",
       " 'Topic segmentation[edit]\\nMain articles: Topic analysis and Document classification\\nTopic analysis consists of two main tasks: topic identiﬁcation and text segmentation.',\n",
       " 'While the first is a simple classification of a specific text, the latter case implies that a document may contain multiple topics, and the task of computerized text segmentation may be to discover these topics automatically and segment the text accordingly.',\n",
       " 'The topic boundaries may be apparent from section titles and paragraphs.',\n",
       " 'In other cases, one needs to use techniques similar to those used in document classification.',\n",
       " 'Segmenting the text into topics or discourse turns might be useful in some natural processing tasks: it can improve information retrieval or speech recognition significantly (by indexing/recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result).',\n",
       " 'It is also needed in topic detection and tracking systems and text summarizing problems.',\n",
       " 'Many different approaches have been tried:[2][3] e.g.',\n",
       " 'HMM, lexical chains, passage similarity using word co-occurrence, clustering, topic modeling, etc.',\n",
       " 'It is quite an ambiguous task – people evaluating the text segmentation systems often differ in topic boundaries.',\n",
       " 'Hence, text segment evaluation is also a challenging problem.',\n",
       " 'Other segmentation problems[edit]\\nProcesses may be required to segment text into segments besides mentioned, including morphemes (a task usually called morphological analysis) or paragraphs.',\n",
       " 'Automatic segmentation approaches[edit]\\nAutomatic segmentation is the problem in natural language processing of implementing a computer process to segment text.',\n",
       " 'When punctuation and similar clues are not consistently available, the segmentation task often requires fairly non-trivial techniques, such as statistical decision-making, large dictionaries, as well as consideration of syntactic and semantic constraints.',\n",
       " 'Effective natural language processing systems and text segmentation tools usually operate on text in specific domains and sources.',\n",
       " 'As an example, processing text used in medical records is a very different problem than processing news articles or real estate advertisements.',\n",
       " 'The process of developing text segmentation tools starts with collecting a large corpus of text in an application domain.',\n",
       " 'There are two general approaches:\\nManual analysis of text and writing custom software\\nAnnotate the sample corpus with boundary information and use machine learning\\nSome text segmentation systems take advantage of any markup like HTML and know document formats like PDF to provide additional evidence for sentence and paragraph boundaries.',\n",
       " 'See also[edit]\\nHyphenation\\nNatural language processing\\nSpeech segmentation\\nLexical analysis\\nWord count\\nLine breaking\\nv\\nt\\ne\\nNatural language processing\\nGeneral terms\\nText corpus\\nSpeech corpus\\nStopwords\\nBag-of-words\\nAI-complete\\nn-gram (Bigram, Trigram)\\nText analysis\\nText segmentation\\nPart-of-speech tagging\\nText chunking\\nCompound term processing\\nCollocation extraction\\nStemming\\nLemmatisation\\nNamed-entity recognition\\nCoreference resolution\\nSentiment analysis\\nConcept mining\\nParsing\\nWord-sense disambiguation\\nTerminology extraction\\nTruecasing\\nAutomatic summarization\\nMulti-document summarization\\nSentence extraction\\nText simplification\\nMachine translation\\nComputer-assisted\\nExample-based\\nRule-based\\nAutomatic identification\\nand data capture\\nSpeech recognition\\nSpeech synthesis\\nOptical character recognition\\nNatural language generation\\nTopic model\\nPachinko allocation\\nLatent Dirichlet allocation\\nLatent semantic analysis\\nComputer-assisted\\nreviewing\\nAutomated essay scoring\\nConcordancer\\nGrammar checker\\nPredictive text\\nSpell checker\\nSyntax guessing\\nNatural language\\nuser interface\\nAutomated online assistant\\nChatbot\\nInteractive fiction\\nQuestion answering\\nReferences[edit]\\n^ UAX #29\\n^ Freddy Y. Y. Choi (2000).',\n",
       " '\"Advances in domain independent linear text segmentation\" (PDF).',\n",
       " 'Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL-00).',\n",
       " 'pp.',\n",
       " '26–33.',\n",
       " '^ Jeffrey C. Reynar (1998).',\n",
       " '\"Topic Segmentation: Algorithms and Applications\" (PDF).',\n",
       " 'IRCS-98-21.',\n",
       " 'University of Pennsylvania.',\n",
       " 'Retrieved 2007-11-08.',\n",
       " 'External links[edit]\\nWord Segment An open source software tool for word segmentation in Chinese.',\n",
       " 'Word Split An open source software tool designed to split conjoined words into human-readable text.',\n",
       " 'Stanford Segmenter An open source software tool for word segmentation in Chinese or morpheme segmentation in Arabic.',\n",
       " 'KyTea An open source software tool for word segmentation in Japanese and Chinese.',\n",
       " 'Chinese Notes A Chinese–English dictionary that also does word segmentation.',\n",
       " 'Zhihuita Segmentor A high precision and high performance Chinese segmentation freeware.',\n",
       " 'Python wordsegment module An open source Python module for English word segmentation.',\n",
       " 'Retrieved from \"https://en.wikipedia.org/w/index.php?title=Text_segmentation&oldid=792337667\"\\nCategories: Tasks of natural language processingHidden categories: Use dmy dates from March 2016Articles needing additional references from October 2011All articles needing additional references\\nNavigation menu\\nPersonal tools\\nNot logged inTalkContributionsCreate accountLog in\\nNamespaces\\nArticle\\nTalk\\nVariants\\nViews\\nRead\\nEdit\\nView history\\nMore\\nSearch\\nNavigation\\nMain pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store\\nInteraction\\nHelpAbout WikipediaCommunity portalRecent changesContact page\\nTools\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page\\nPrint/export\\nCreate a bookDownload as PDFPrintable version\\nLanguages\\nবাংলাDeutschGalegoՀայերենBahasa Indonesia日本語Norsk nynorskPortuguêsРусский\\nEdit links\\nThis page was last edited on 25 July 2017, at 22:51.',\n",
       " 'Text is available under the Creative Commons Attribution-ShareAlike License;\\nadditional terms may apply.',\n",
       " 'By using this site, you agree to the Terms of Use and Privacy Policy.',\n",
       " 'Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.',\n",
       " 'Privacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nDevelopers\\nCookie statement\\nMobile view']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text segmentation - Wikipedia\n",
      "Text segmentation\n",
      "From Wikipedia, the free encyclopedia\n",
      "Jump to:\t\t\t\t\tnavigation, \t\t\t\t\tsearch\n",
      "This article needs additional citations for verification.\n",
      "Please help improve this article by adding citations to reliable sources.\n",
      "Unsourced material may be challenged and removed.\n",
      "(October 2011) (Learn how and when to remove this template message)\n",
      "Text segmentation is the process of dividing written text into meaningful units, such as words, sentences, or topics.\n",
      "The term applies both to mental processes used by humans when reading text, and to artificial processes implemented in computers, which are the subject of natural language processing.\n",
      "The problem is non-trivial, because while some written languages have explicit word boundary markers, such as the word spaces of written English and the distinctive initial, medial and final letter shapes of Arabic, such signals are sometimes ambiguous and not present in all written languages.\n",
      "Compare speech segmentation, the process of dividing speech into linguistically meaningful portions.\n",
      "Contents\n",
      "1 Segmentation problems\n",
      "1.1 Word segmentation\n",
      "1.2 Intent segmentation\n",
      "1.3 Sentence segmentation\n",
      "1.4 Topic segmentation\n",
      "1.5 Other segmentation problems\n",
      "2 Automatic segmentation approaches\n",
      "3 See also\n",
      "4 References\n",
      "5 External links\n",
      "Segmentation problems[edit]\n",
      "Word segmentation[edit]\n",
      "See also: Word § Word boundaries\n",
      "Word segmentation is the problem of dividing a string of written language into its component words.\n",
      "In English and many other languages using some form of the Latin alphabet, the space is a good approximation of a word divider (word delimiter), although this concept has limits because of the variability with which languages emically regard collocations and compounds.\n",
      "Many English compound nouns are variably written (for example, ice box = ice-box = icebox; pig sty = pig-sty = pigsty) with a corresponding variation in whether speakers think of them as noun phrases or single nouns; there are trends in how norms are set, such as that open compounds often tend eventually to solidify by widespread convention, but variation remains systemic.\n",
      "In contrast, German compound nouns show less orthographic variation, with solidification being a stronger norm.\n",
      "However, the equivalent to the word space character is not found in all written scripts, and without it word segmentation is a difficult problem.\n",
      "Languages which do not have a trivial word segmentation process include Chinese, Japanese, where sentences but not words are delimited, Thai and Lao, where phrases and sentences but not words are delimited, and Vietnamese, where syllables but not words are delimited.\n",
      "In some writing systems however, such as the Ge'ez script used for Amharic and Tigrinya among other languages, words are explicitly delimited (at least historically) with a non-whitespace character.\n",
      "The Unicode Consortium has published a Standard Annex on Text Segmentation,[1] exploring the issues of segmentation in multiscript texts.\n",
      "Word splitting is the process of parsing concatenated text (i.e.\n",
      "text that contains no spaces or other word separators) to infer where word breaks exist.\n",
      "Word splitting may also refer to the process of hyphenation.\n",
      "Intent segmentation[edit]\n",
      "See also: Tri-box method\n",
      "Intent segmentation is the problem of dividing written words into keyphrases (2 or more group of words).\n",
      "In English and all other languages the core intent or desire is identified and become the corner-stone of the keyphrase Intent segmentation.\n",
      "Core product/service, idea, action & or thought anchor the keyphrase.\n",
      "\"[All things are made of atoms].\n",
      "[Little particles that move] [around in perpetual motion], [attraction each other] [when they are a little distance apart], [but repelling] [upon being squeezed] [into one another].\"\n",
      "Sentence segmentation[edit]\n",
      "See also: Sentence boundary disambiguation\n",
      "Sentence segmentation is the problem of dividing a string of written language into its component sentences.\n",
      "In English and some other languages, using punctuation, particularly the full stop/period character is a reasonable approximation.\n",
      "However even in English this problem is not trivial due to the use of the full stop character for abbreviations, which may or may not also terminate a sentence.\n",
      "For example Mr. is not its own sentence in \"Mr. Smith went to the shops in Jones Street.\"\n",
      "When processing plain text, tables of abbreviations that contain periods can help prevent incorrect assignment of sentence boundaries.\n",
      "As with word segmentation, not all written languages contain punctuation characters which are useful for approximating sentence boundaries.\n",
      "Topic segmentation[edit]\n",
      "Main articles: Topic analysis and Document classification\n",
      "Topic analysis consists of two main tasks: topic identiﬁcation and text segmentation.\n",
      "While the first is a simple classification of a specific text, the latter case implies that a document may contain multiple topics, and the task of computerized text segmentation may be to discover these topics automatically and segment the text accordingly.\n",
      "The topic boundaries may be apparent from section titles and paragraphs.\n",
      "In other cases, one needs to use techniques similar to those used in document classification.\n",
      "Segmenting the text into topics or discourse turns might be useful in some natural processing tasks: it can improve information retrieval or speech recognition significantly (by indexing/recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result).\n",
      "It is also needed in topic detection and tracking systems and text summarizing problems.\n",
      "Many different approaches have been tried:[2][3] e.g.\n",
      "HMM, lexical chains, passage similarity using word co-occurrence, clustering, topic modeling, etc.\n",
      "It is quite an ambiguous task – people evaluating the text segmentation systems often differ in topic boundaries.\n",
      "Hence, text segment evaluation is also a challenging problem.\n",
      "Other segmentation problems[edit]\n",
      "Processes may be required to segment text into segments besides mentioned, including morphemes (a task usually called morphological analysis) or paragraphs.\n",
      "Automatic segmentation approaches[edit]\n",
      "Automatic segmentation is the problem in natural language processing of implementing a computer process to segment text.\n",
      "When punctuation and similar clues are not consistently available, the segmentation task often requires fairly non-trivial techniques, such as statistical decision-making, large dictionaries, as well as consideration of syntactic and semantic constraints.\n",
      "Effective natural language processing systems and text segmentation tools usually operate on text in specific domains and sources.\n",
      "As an example, processing text used in medical records is a very different problem than processing news articles or real estate advertisements.\n",
      "The process of developing text segmentation tools starts with collecting a large corpus of text in an application domain.\n",
      "There are two general approaches:\n",
      "Manual analysis of text and writing custom software\n",
      "Annotate the sample corpus with boundary information and use machine learning\n",
      "Some text segmentation systems take advantage of any markup like HTML and know document formats like PDF to provide additional evidence for sentence and paragraph boundaries.\n",
      "See also[edit]\n",
      "Hyphenation\n",
      "Natural language processing\n",
      "Speech segmentation\n",
      "Lexical analysis\n",
      "Word count\n",
      "Line breaking\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural language processing\n",
      "General terms\n",
      "Text corpus\n",
      "Speech corpus\n",
      "Stopwords\n",
      "Bag-of-words\n",
      "AI-complete\n",
      "n-gram (Bigram, Trigram)\n",
      "Text analysis\n",
      "Text segmentation\n",
      "Part-of-speech tagging\n",
      "Text chunking\n",
      "Compound term processing\n",
      "Collocation extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "Named-entity recognition\n",
      "Coreference resolution\n",
      "Sentiment analysis\n",
      "Concept mining\n",
      "Parsing\n",
      "Word-sense disambiguation\n",
      "Terminology extraction\n",
      "Truecasing\n",
      "Automatic summarization\n",
      "Multi-document summarization\n",
      "Sentence extraction\n",
      "Text simplification\n",
      "Machine translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Automatic identification\n",
      "and data capture\n",
      "Speech recognition\n",
      "Speech synthesis\n",
      "Optical character recognition\n",
      "Natural language generation\n",
      "Topic model\n",
      "Pachinko allocation\n",
      "Latent Dirichlet allocation\n",
      "Latent semantic analysis\n",
      "Computer-assisted\n",
      "reviewing\n",
      "Automated essay scoring\n",
      "Concordancer\n",
      "Grammar checker\n",
      "Predictive text\n",
      "Spell checker\n",
      "Syntax guessing\n",
      "Natural language\n",
      "user interface\n",
      "Automated online assistant\n",
      "Chatbot\n",
      "Interactive fiction\n",
      "Question answering\n",
      "References[edit]\n",
      "^ UAX #29\n",
      "^ Freddy Y. Y. Choi (2000).\n",
      "\"Advances in domain independent linear text segmentation\" (PDF).\n",
      "Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (ANLP-NAACL-00).\n",
      "pp.\n",
      "26–33.\n",
      "^ Jeffrey C. Reynar (1998).\n",
      "\"Topic Segmentation: Algorithms and Applications\" (PDF).\n",
      "IRCS-98-21.\n",
      "University of Pennsylvania.\n",
      "Retrieved 2007-11-08.\n",
      "External links[edit]\n",
      "Word Segment An open source software tool for word segmentation in Chinese.\n",
      "Word Split An open source software tool designed to split conjoined words into human-readable text.\n",
      "Stanford Segmenter An open source software tool for word segmentation in Chinese or morpheme segmentation in Arabic.\n",
      "KyTea An open source software tool for word segmentation in Japanese and Chinese.\n",
      "Chinese Notes A Chinese–English dictionary that also does word segmentation.\n",
      "Zhihuita Segmentor A high precision and high performance Chinese segmentation freeware.\n",
      "Python wordsegment module An open source Python module for English word segmentation.\n",
      "Retrieved from \"https://en.wikipedia.org/w/index.php?title=Text_segmentation&oldid=792337667\"\n",
      "Categories: Tasks of natural language processingHidden categories: Use dmy dates from March 2016Articles needing additional references from October 2011All articles needing additional references\n",
      "Navigation menu\n",
      "Personal tools\n",
      "Not logged inTalkContributionsCreate accountLog in\n",
      "Namespaces\n",
      "Article\n",
      "Talk\n",
      "Variants\n",
      "Views\n",
      "Read\n",
      "Edit\n",
      "View history\n",
      "More\n",
      "Search\n",
      "Navigation\n",
      "Main pageContentsFeatured contentCurrent eventsRandom articleDonate to WikipediaWikipedia store\n",
      "Interaction\n",
      "HelpAbout WikipediaCommunity portalRecent changesContact page\n",
      "Tools\n",
      "What links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationWikidata itemCite this page\n",
      "Print/export\n",
      "Create a bookDownload as PDFPrintable version\n",
      "Languages\n",
      "বাংলাDeutschGalegoՀայերենBahasa Indonesia日本語Norsk nynorskPortuguêsРусский\n",
      "Edit links\n",
      "This page was last edited on 25 July 2017, at 22:51.\n",
      "Text is available under the Creative Commons Attribution-ShareAlike License;\n",
      "additional terms may apply.\n",
      "By using this site, you agree to the Terms of Use and Privacy Policy.\n",
      "Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n",
      "Privacy policy\n",
      "About Wikipedia\n",
      "Disclaimers\n",
      "Contact Wikipedia\n",
      "Developers\n",
      "Cookie statement\n",
      "Mobile view\n"
     ]
    }
   ],
   "source": [
    "for i in sent_tokenize_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'segmentation',\n",
       " '-',\n",
       " 'Wikipedia',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'From',\n",
       " 'Wikipedia',\n",
       " ',',\n",
       " 'the',\n",
       " 'free',\n",
       " 'encyclopedia',\n",
       " 'Jump',\n",
       " 'to',\n",
       " ':',\n",
       " 'navigation',\n",
       " ',',\n",
       " 'search',\n",
       " 'This',\n",
       " 'article',\n",
       " 'needs',\n",
       " 'additional',\n",
       " 'citations',\n",
       " 'for',\n",
       " 'verification',\n",
       " '.',\n",
       " 'Please',\n",
       " 'help',\n",
       " 'improve',\n",
       " 'this',\n",
       " 'article',\n",
       " 'by',\n",
       " 'adding',\n",
       " 'citations',\n",
       " 'to',\n",
       " 'reliable',\n",
       " 'sources',\n",
       " '.',\n",
       " 'Unsourced',\n",
       " 'material',\n",
       " 'may',\n",
       " 'be',\n",
       " 'challenged',\n",
       " 'and',\n",
       " 'removed',\n",
       " '.',\n",
       " '(',\n",
       " 'October',\n",
       " '2011',\n",
       " ')',\n",
       " '(',\n",
       " 'Learn',\n",
       " 'how',\n",
       " 'and',\n",
       " 'when',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'this',\n",
       " 'template',\n",
       " 'message',\n",
       " ')',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'text',\n",
       " 'into',\n",
       " 'meaningful',\n",
       " 'units',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'or',\n",
       " 'topics',\n",
       " '.',\n",
       " 'The',\n",
       " 'term',\n",
       " 'applies',\n",
       " 'both',\n",
       " 'to',\n",
       " 'mental',\n",
       " 'processes',\n",
       " 'used',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'when',\n",
       " 'reading',\n",
       " 'text',\n",
       " ',',\n",
       " 'and',\n",
       " 'to',\n",
       " 'artificial',\n",
       " 'processes',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'computers',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'non-trivial',\n",
       " ',',\n",
       " 'because',\n",
       " 'while',\n",
       " 'some',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'have',\n",
       " 'explicit',\n",
       " 'word',\n",
       " 'boundary',\n",
       " 'markers',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'word',\n",
       " 'spaces',\n",
       " 'of',\n",
       " 'written',\n",
       " 'English',\n",
       " 'and',\n",
       " 'the',\n",
       " 'distinctive',\n",
       " 'initial',\n",
       " ',',\n",
       " 'medial',\n",
       " 'and',\n",
       " 'final',\n",
       " 'letter',\n",
       " 'shapes',\n",
       " 'of',\n",
       " 'Arabic',\n",
       " ',',\n",
       " 'such',\n",
       " 'signals',\n",
       " 'are',\n",
       " 'sometimes',\n",
       " 'ambiguous',\n",
       " 'and',\n",
       " 'not',\n",
       " 'present',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages',\n",
       " '.',\n",
       " 'Compare',\n",
       " 'speech',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'speech',\n",
       " 'into',\n",
       " 'linguistically',\n",
       " 'meaningful',\n",
       " 'portions',\n",
       " '.',\n",
       " 'Contents',\n",
       " '1',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '1.1',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '1.2',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '1.3',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '1.4',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '1.5',\n",
       " 'Other',\n",
       " 'segmentation',\n",
       " 'problems',\n",
       " '2',\n",
       " 'Automatic',\n",
       " 'segmentation',\n",
       " 'approaches',\n",
       " '3',\n",
       " 'See',\n",
       " 'also',\n",
       " '4',\n",
       " 'References',\n",
       " '5',\n",
       " 'External',\n",
       " 'links',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Word',\n",
       " '§',\n",
       " 'Word',\n",
       " 'boundaries',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'words',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'many',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'using',\n",
       " 'some',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Latin',\n",
       " 'alphabet',\n",
       " ',',\n",
       " 'the',\n",
       " 'space',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'divider',\n",
       " '(',\n",
       " 'word',\n",
       " 'delimiter',\n",
       " ')',\n",
       " ',',\n",
       " 'although',\n",
       " 'this',\n",
       " 'concept',\n",
       " 'has',\n",
       " 'limits',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'with',\n",
       " 'which',\n",
       " 'languages',\n",
       " 'emically',\n",
       " 'regard',\n",
       " 'collocations',\n",
       " 'and',\n",
       " 'compounds',\n",
       " '.',\n",
       " 'Many',\n",
       " 'English',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'are',\n",
       " 'variably',\n",
       " 'written',\n",
       " '(',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'ice',\n",
       " 'box',\n",
       " '=',\n",
       " 'ice-box',\n",
       " '=',\n",
       " 'icebox',\n",
       " ';',\n",
       " 'pig',\n",
       " 'sty',\n",
       " '=',\n",
       " 'pig-sty',\n",
       " '=',\n",
       " 'pigsty',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'corresponding',\n",
       " 'variation',\n",
       " 'in',\n",
       " 'whether',\n",
       " 'speakers',\n",
       " 'think',\n",
       " 'of',\n",
       " 'them',\n",
       " 'as',\n",
       " 'noun',\n",
       " 'phrases',\n",
       " 'or',\n",
       " 'single',\n",
       " 'nouns',\n",
       " ';',\n",
       " 'there',\n",
       " 'are',\n",
       " 'trends',\n",
       " 'in',\n",
       " 'how',\n",
       " 'norms',\n",
       " 'are',\n",
       " 'set',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'that',\n",
       " 'open',\n",
       " 'compounds',\n",
       " 'often',\n",
       " 'tend',\n",
       " 'eventually',\n",
       " 'to',\n",
       " 'solidify',\n",
       " 'by',\n",
       " 'widespread',\n",
       " 'convention',\n",
       " ',',\n",
       " 'but',\n",
       " 'variation',\n",
       " 'remains',\n",
       " 'systemic',\n",
       " '.',\n",
       " 'In',\n",
       " 'contrast',\n",
       " ',',\n",
       " 'German',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'show',\n",
       " 'less',\n",
       " 'orthographic',\n",
       " 'variation',\n",
       " ',',\n",
       " 'with',\n",
       " 'solidification',\n",
       " 'being',\n",
       " 'a',\n",
       " 'stronger',\n",
       " 'norm',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'the',\n",
       " 'equivalent',\n",
       " 'to',\n",
       " 'the',\n",
       " 'word',\n",
       " 'space',\n",
       " 'character',\n",
       " 'is',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'scripts',\n",
       " ',',\n",
       " 'and',\n",
       " 'without',\n",
       " 'it',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'a',\n",
       " 'difficult',\n",
       " 'problem',\n",
       " '.',\n",
       " 'Languages',\n",
       " 'which',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'a',\n",
       " 'trivial',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'process',\n",
       " 'include',\n",
       " 'Chinese',\n",
       " ',',\n",
       " 'Japanese',\n",
       " ',',\n",
       " 'where',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'Thai',\n",
       " 'and',\n",
       " 'Lao',\n",
       " ',',\n",
       " 'where',\n",
       " 'phrases',\n",
       " 'and',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'and',\n",
       " 'Vietnamese',\n",
       " ',',\n",
       " 'where',\n",
       " 'syllables',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'writing',\n",
       " 'systems',\n",
       " 'however',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " \"Ge'ez\",\n",
       " 'script',\n",
       " 'used',\n",
       " 'for',\n",
       " 'Amharic',\n",
       " 'and',\n",
       " 'Tigrinya',\n",
       " 'among',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'words',\n",
       " 'are',\n",
       " 'explicitly',\n",
       " 'delimited',\n",
       " '(',\n",
       " 'at',\n",
       " 'least',\n",
       " 'historically',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'non-whitespace',\n",
       " 'character',\n",
       " '.',\n",
       " 'The',\n",
       " 'Unicode',\n",
       " 'Consortium',\n",
       " 'has',\n",
       " 'published',\n",
       " 'a',\n",
       " 'Standard',\n",
       " 'Annex',\n",
       " 'on',\n",
       " 'Text',\n",
       " 'Segmentation',\n",
       " ',',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'issues',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'in',\n",
       " 'multiscript',\n",
       " 'texts',\n",
       " '.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'parsing',\n",
       " 'concatenated',\n",
       " 'text',\n",
       " '(',\n",
       " 'i.e',\n",
       " '.',\n",
       " 'text',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'no',\n",
       " 'spaces',\n",
       " 'or',\n",
       " 'other',\n",
       " 'word',\n",
       " 'separators',\n",
       " ')',\n",
       " 'to',\n",
       " 'infer',\n",
       " 'where',\n",
       " 'word',\n",
       " 'breaks',\n",
       " 'exist',\n",
       " '.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'may',\n",
       " 'also',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'hyphenation',\n",
       " '.',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Tri-box',\n",
       " 'method',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'words',\n",
       " 'into',\n",
       " 'keyphrases',\n",
       " '(',\n",
       " '2',\n",
       " 'or',\n",
       " 'more',\n",
       " 'group',\n",
       " 'of',\n",
       " 'words',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'all',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'the',\n",
       " 'core',\n",
       " 'intent',\n",
       " 'or',\n",
       " 'desire',\n",
       " 'is',\n",
       " 'identified',\n",
       " 'and',\n",
       " 'become',\n",
       " 'the',\n",
       " 'corner-stone',\n",
       " 'of',\n",
       " 'the',\n",
       " 'keyphrase',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '.',\n",
       " 'Core',\n",
       " 'product/service',\n",
       " ',',\n",
       " 'idea',\n",
       " ',',\n",
       " 'action',\n",
       " '&',\n",
       " 'or',\n",
       " 'thought',\n",
       " 'anchor',\n",
       " 'the',\n",
       " 'keyphrase',\n",
       " '.',\n",
       " '``',\n",
       " '[',\n",
       " 'All',\n",
       " 'things',\n",
       " 'are',\n",
       " 'made',\n",
       " 'of',\n",
       " 'atoms',\n",
       " ']',\n",
       " '.',\n",
       " '[',\n",
       " 'Little',\n",
       " 'particles',\n",
       " 'that',\n",
       " 'move',\n",
       " ']',\n",
       " '[',\n",
       " 'around',\n",
       " 'in',\n",
       " 'perpetual',\n",
       " 'motion',\n",
       " ']',\n",
       " ',',\n",
       " '[',\n",
       " 'attraction',\n",
       " 'each',\n",
       " 'other',\n",
       " ']',\n",
       " '[',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'a',\n",
       " 'little',\n",
       " 'distance',\n",
       " 'apart',\n",
       " ']',\n",
       " ',',\n",
       " '[',\n",
       " 'but',\n",
       " 'repelling',\n",
       " ']',\n",
       " '[',\n",
       " 'upon',\n",
       " 'being',\n",
       " 'squeezed',\n",
       " ']',\n",
       " '[',\n",
       " 'into',\n",
       " 'one',\n",
       " 'another',\n",
       " ']',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Sentence',\n",
       " 'boundary',\n",
       " 'disambiguation',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'some',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'using',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop/period',\n",
       " 'character',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reasonable',\n",
       " 'approximation',\n",
       " '.',\n",
       " 'However',\n",
       " 'even',\n",
       " 'in',\n",
       " 'English',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'character',\n",
       " 'for',\n",
       " 'abbreviations',\n",
       " ',',\n",
       " 'which',\n",
       " 'may',\n",
       " 'or',\n",
       " 'may',\n",
       " 'not',\n",
       " 'also',\n",
       " 'terminate',\n",
       " 'a',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'Mr.',\n",
       " 'is',\n",
       " 'not',\n",
       " 'its',\n",
       " 'own',\n",
       " 'sentence',\n",
       " 'in',\n",
       " '``',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'shops',\n",
       " 'in',\n",
       " 'Jones',\n",
       " 'Street',\n",
       " '.',\n",
       " \"''\",\n",
       " 'When',\n",
       " 'processing',\n",
       " 'plain',\n",
       " 'text',\n",
       " ',',\n",
       " 'tables',\n",
       " 'of',\n",
       " 'abbreviations',\n",
       " 'that',\n",
       " 'contain',\n",
       " 'periods',\n",
       " 'can',\n",
       " 'help',\n",
       " 'prevent',\n",
       " 'incorrect',\n",
       " 'assignment',\n",
       " 'of',\n",
       " 'sentence',\n",
       " 'boundaries',\n",
       " '.',\n",
       " 'As',\n",
       " 'with',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'not',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'contain',\n",
       " 'punctuation',\n",
       " 'characters',\n",
       " 'which',\n",
       " 'are',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'approximating',\n",
       " 'sentence',\n",
       " 'boundaries',\n",
       " '.',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Main',\n",
       " 'articles',\n",
       " ':',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'Document',\n",
       " 'classification',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'two',\n",
       " 'main',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'topic',\n",
       " 'identiﬁcation',\n",
       " 'and',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " '.',\n",
       " 'While',\n",
       " 'the',\n",
       " 'first',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'classification',\n",
       " 'of',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'text',\n",
       " ',',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'case',\n",
       " 'implies',\n",
       " 'that',\n",
       " 'a',\n",
       " 'document',\n",
       " 'may',\n",
       " 'contain',\n",
       " 'multiple',\n",
       " 'topics',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'computerized',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'these',\n",
       " 'topics',\n",
       " 'automatically',\n",
       " 'and',\n",
       " 'segment',\n",
       " 'the',\n",
       " 'text',\n",
       " 'accordingly',\n",
       " '.',\n",
       " 'The',\n",
       " 'topic',\n",
       " 'boundaries',\n",
       " 'may',\n",
       " 'be',\n",
       " 'apparent',\n",
       " 'from',\n",
       " 'section',\n",
       " 'titles',\n",
       " 'and',\n",
       " 'paragraphs',\n",
       " '.',\n",
       " 'In',\n",
       " 'other',\n",
       " 'cases',\n",
       " ',',\n",
       " 'one',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'use',\n",
       " 'techniques',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'those',\n",
       " 'used',\n",
       " 'in',\n",
       " 'document',\n",
       " 'classification',\n",
       " '.',\n",
       " 'Segmenting',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'topics',\n",
       " 'or',\n",
       " 'discourse',\n",
       " 'turns',\n",
       " 'might',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'some',\n",
       " 'natural',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'it',\n",
       " 'can',\n",
       " 'improve',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'or',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'significantly',\n",
       " '(',\n",
       " 'by',\n",
       " 'indexing/recognizing',\n",
       " 'documents',\n",
       " 'more',\n",
       " 'precisely',\n",
       " 'or',\n",
       " 'by',\n",
       " 'giving',\n",
       " 'the',\n",
       " 'specific',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'document',\n",
       " 'corresponding',\n",
       " 'to',\n",
       " 'the',\n",
       " 'query',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'also',\n",
       " 'needed',\n",
       " 'in',\n",
       " 'topic',\n",
       " 'detection',\n",
       " 'and',\n",
       " 'tracking',\n",
       " 'systems',\n",
       " 'and',\n",
       " 'text',\n",
       " 'summarizing',\n",
       " 'problems',\n",
       " '.',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'approaches',\n",
       " 'have',\n",
       " 'been',\n",
       " 'tried',\n",
       " ':',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '[',\n",
       " '3',\n",
       " ']',\n",
       " 'e.g',\n",
       " '.',\n",
       " 'HMM',\n",
       " ',',\n",
       " 'lexical',\n",
       " 'chains',\n",
       " ',',\n",
       " 'passage',\n",
       " 'similarity',\n",
       " 'using',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WORD TOKENIZER\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'segmentation',\n",
       " '-',\n",
       " 'Wikipedia',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'From',\n",
       " 'Wikipedia',\n",
       " ',',\n",
       " 'the',\n",
       " 'free',\n",
       " 'encyclopedia',\n",
       " 'Jump',\n",
       " 'to',\n",
       " ':',\n",
       " 'navigation',\n",
       " ',',\n",
       " 'search',\n",
       " 'This',\n",
       " 'article',\n",
       " 'needs',\n",
       " 'additional',\n",
       " 'citations',\n",
       " 'for',\n",
       " 'verification.',\n",
       " 'Please',\n",
       " 'help',\n",
       " 'improve',\n",
       " 'this',\n",
       " 'article',\n",
       " 'by',\n",
       " 'adding',\n",
       " 'citations',\n",
       " 'to',\n",
       " 'reliable',\n",
       " 'sources.',\n",
       " 'Unsourced',\n",
       " 'material',\n",
       " 'may',\n",
       " 'be',\n",
       " 'challenged',\n",
       " 'and',\n",
       " 'removed.',\n",
       " '(',\n",
       " 'October',\n",
       " '2011',\n",
       " ')',\n",
       " '(',\n",
       " 'Learn',\n",
       " 'how',\n",
       " 'and',\n",
       " 'when',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'this',\n",
       " 'template',\n",
       " 'message',\n",
       " ')',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'text',\n",
       " 'into',\n",
       " 'meaningful',\n",
       " 'units',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'or',\n",
       " 'topics.',\n",
       " 'The',\n",
       " 'term',\n",
       " 'applies',\n",
       " 'both',\n",
       " 'to',\n",
       " 'mental',\n",
       " 'processes',\n",
       " 'used',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'when',\n",
       " 'reading',\n",
       " 'text',\n",
       " ',',\n",
       " 'and',\n",
       " 'to',\n",
       " 'artificial',\n",
       " 'processes',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'computers',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'non-trivial',\n",
       " ',',\n",
       " 'because',\n",
       " 'while',\n",
       " 'some',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'have',\n",
       " 'explicit',\n",
       " 'word',\n",
       " 'boundary',\n",
       " 'markers',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'word',\n",
       " 'spaces',\n",
       " 'of',\n",
       " 'written',\n",
       " 'English',\n",
       " 'and',\n",
       " 'the',\n",
       " 'distinctive',\n",
       " 'initial',\n",
       " ',',\n",
       " 'medial',\n",
       " 'and',\n",
       " 'final',\n",
       " 'letter',\n",
       " 'shapes',\n",
       " 'of',\n",
       " 'Arabic',\n",
       " ',',\n",
       " 'such',\n",
       " 'signals',\n",
       " 'are',\n",
       " 'sometimes',\n",
       " 'ambiguous',\n",
       " 'and',\n",
       " 'not',\n",
       " 'present',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages.',\n",
       " 'Compare',\n",
       " 'speech',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'speech',\n",
       " 'into',\n",
       " 'linguistically',\n",
       " 'meaningful',\n",
       " 'portions.',\n",
       " 'Contents',\n",
       " '1',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '1.1',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '1.2',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '1.3',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '1.4',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '1.5',\n",
       " 'Other',\n",
       " 'segmentation',\n",
       " 'problems',\n",
       " '2',\n",
       " 'Automatic',\n",
       " 'segmentation',\n",
       " 'approaches',\n",
       " '3',\n",
       " 'See',\n",
       " 'also',\n",
       " '4',\n",
       " 'References',\n",
       " '5',\n",
       " 'External',\n",
       " 'links',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Word',\n",
       " '§',\n",
       " 'Word',\n",
       " 'boundaries',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'words.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'many',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'using',\n",
       " 'some',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Latin',\n",
       " 'alphabet',\n",
       " ',',\n",
       " 'the',\n",
       " 'space',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'divider',\n",
       " '(',\n",
       " 'word',\n",
       " 'delimiter',\n",
       " ')',\n",
       " ',',\n",
       " 'although',\n",
       " 'this',\n",
       " 'concept',\n",
       " 'has',\n",
       " 'limits',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'with',\n",
       " 'which',\n",
       " 'languages',\n",
       " 'emically',\n",
       " 'regard',\n",
       " 'collocations',\n",
       " 'and',\n",
       " 'compounds.',\n",
       " 'Many',\n",
       " 'English',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'are',\n",
       " 'variably',\n",
       " 'written',\n",
       " '(',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'ice',\n",
       " 'box',\n",
       " '=',\n",
       " 'ice-box',\n",
       " '=',\n",
       " 'icebox',\n",
       " ';',\n",
       " 'pig',\n",
       " 'sty',\n",
       " '=',\n",
       " 'pig-sty',\n",
       " '=',\n",
       " 'pigsty',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'corresponding',\n",
       " 'variation',\n",
       " 'in',\n",
       " 'whether',\n",
       " 'speakers',\n",
       " 'think',\n",
       " 'of',\n",
       " 'them',\n",
       " 'as',\n",
       " 'noun',\n",
       " 'phrases',\n",
       " 'or',\n",
       " 'single',\n",
       " 'nouns',\n",
       " ';',\n",
       " 'there',\n",
       " 'are',\n",
       " 'trends',\n",
       " 'in',\n",
       " 'how',\n",
       " 'norms',\n",
       " 'are',\n",
       " 'set',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'that',\n",
       " 'open',\n",
       " 'compounds',\n",
       " 'often',\n",
       " 'tend',\n",
       " 'eventually',\n",
       " 'to',\n",
       " 'solidify',\n",
       " 'by',\n",
       " 'widespread',\n",
       " 'convention',\n",
       " ',',\n",
       " 'but',\n",
       " 'variation',\n",
       " 'remains',\n",
       " 'systemic.',\n",
       " 'In',\n",
       " 'contrast',\n",
       " ',',\n",
       " 'German',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'show',\n",
       " 'less',\n",
       " 'orthographic',\n",
       " 'variation',\n",
       " ',',\n",
       " 'with',\n",
       " 'solidification',\n",
       " 'being',\n",
       " 'a',\n",
       " 'stronger',\n",
       " 'norm.',\n",
       " 'However',\n",
       " ',',\n",
       " 'the',\n",
       " 'equivalent',\n",
       " 'to',\n",
       " 'the',\n",
       " 'word',\n",
       " 'space',\n",
       " 'character',\n",
       " 'is',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'scripts',\n",
       " ',',\n",
       " 'and',\n",
       " 'without',\n",
       " 'it',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'a',\n",
       " 'difficult',\n",
       " 'problem.',\n",
       " 'Languages',\n",
       " 'which',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'a',\n",
       " 'trivial',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'process',\n",
       " 'include',\n",
       " 'Chinese',\n",
       " ',',\n",
       " 'Japanese',\n",
       " ',',\n",
       " 'where',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'Thai',\n",
       " 'and',\n",
       " 'Lao',\n",
       " ',',\n",
       " 'where',\n",
       " 'phrases',\n",
       " 'and',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'and',\n",
       " 'Vietnamese',\n",
       " ',',\n",
       " 'where',\n",
       " 'syllables',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'writing',\n",
       " 'systems',\n",
       " 'however',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " \"Ge'ez\",\n",
       " 'script',\n",
       " 'used',\n",
       " 'for',\n",
       " 'Amharic',\n",
       " 'and',\n",
       " 'Tigrinya',\n",
       " 'among',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'words',\n",
       " 'are',\n",
       " 'explicitly',\n",
       " 'delimited',\n",
       " '(',\n",
       " 'at',\n",
       " 'least',\n",
       " 'historically',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'non-whitespace',\n",
       " 'character.',\n",
       " 'The',\n",
       " 'Unicode',\n",
       " 'Consortium',\n",
       " 'has',\n",
       " 'published',\n",
       " 'a',\n",
       " 'Standard',\n",
       " 'Annex',\n",
       " 'on',\n",
       " 'Text',\n",
       " 'Segmentation',\n",
       " ',',\n",
       " '[',\n",
       " '1',\n",
       " ']',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'issues',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'in',\n",
       " 'multiscript',\n",
       " 'texts.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'parsing',\n",
       " 'concatenated',\n",
       " 'text',\n",
       " '(',\n",
       " 'i.e.',\n",
       " 'text',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'no',\n",
       " 'spaces',\n",
       " 'or',\n",
       " 'other',\n",
       " 'word',\n",
       " 'separators',\n",
       " ')',\n",
       " 'to',\n",
       " 'infer',\n",
       " 'where',\n",
       " 'word',\n",
       " 'breaks',\n",
       " 'exist.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'may',\n",
       " 'also',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'hyphenation.',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Tri-box',\n",
       " 'method',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'words',\n",
       " 'into',\n",
       " 'keyphrases',\n",
       " '(',\n",
       " '2',\n",
       " 'or',\n",
       " 'more',\n",
       " 'group',\n",
       " 'of',\n",
       " 'words',\n",
       " ')',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'all',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'the',\n",
       " 'core',\n",
       " 'intent',\n",
       " 'or',\n",
       " 'desire',\n",
       " 'is',\n",
       " 'identified',\n",
       " 'and',\n",
       " 'become',\n",
       " 'the',\n",
       " 'corner-stone',\n",
       " 'of',\n",
       " 'the',\n",
       " 'keyphrase',\n",
       " 'Intent',\n",
       " 'segmentation.',\n",
       " 'Core',\n",
       " 'product/service',\n",
       " ',',\n",
       " 'idea',\n",
       " ',',\n",
       " 'action',\n",
       " '&',\n",
       " 'or',\n",
       " 'thought',\n",
       " 'anchor',\n",
       " 'the',\n",
       " 'keyphrase.',\n",
       " \"''\",\n",
       " '[',\n",
       " 'All',\n",
       " 'things',\n",
       " 'are',\n",
       " 'made',\n",
       " 'of',\n",
       " 'atoms',\n",
       " ']',\n",
       " '.',\n",
       " '[',\n",
       " 'Little',\n",
       " 'particles',\n",
       " 'that',\n",
       " 'move',\n",
       " ']',\n",
       " '[',\n",
       " 'around',\n",
       " 'in',\n",
       " 'perpetual',\n",
       " 'motion',\n",
       " ']',\n",
       " ',',\n",
       " '[',\n",
       " 'attraction',\n",
       " 'each',\n",
       " 'other',\n",
       " ']',\n",
       " '[',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'a',\n",
       " 'little',\n",
       " 'distance',\n",
       " 'apart',\n",
       " ']',\n",
       " ',',\n",
       " '[',\n",
       " 'but',\n",
       " 'repelling',\n",
       " ']',\n",
       " '[',\n",
       " 'upon',\n",
       " 'being',\n",
       " 'squeezed',\n",
       " ']',\n",
       " '[',\n",
       " 'into',\n",
       " 'one',\n",
       " 'another',\n",
       " ']',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Sentence',\n",
       " 'boundary',\n",
       " 'disambiguation',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'sentences.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'some',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'using',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop/period',\n",
       " 'character',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reasonable',\n",
       " 'approximation.',\n",
       " 'However',\n",
       " 'even',\n",
       " 'in',\n",
       " 'English',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'character',\n",
       " 'for',\n",
       " 'abbreviations',\n",
       " ',',\n",
       " 'which',\n",
       " 'may',\n",
       " 'or',\n",
       " 'may',\n",
       " 'not',\n",
       " 'also',\n",
       " 'terminate',\n",
       " 'a',\n",
       " 'sentence.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'Mr.',\n",
       " 'is',\n",
       " 'not',\n",
       " 'its',\n",
       " 'own',\n",
       " 'sentence',\n",
       " 'in',\n",
       " '``',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'shops',\n",
       " 'in',\n",
       " 'Jones',\n",
       " 'Street.',\n",
       " \"''\",\n",
       " 'When',\n",
       " 'processing',\n",
       " 'plain',\n",
       " 'text',\n",
       " ',',\n",
       " 'tables',\n",
       " 'of',\n",
       " 'abbreviations',\n",
       " 'that',\n",
       " 'contain',\n",
       " 'periods',\n",
       " 'can',\n",
       " 'help',\n",
       " 'prevent',\n",
       " 'incorrect',\n",
       " 'assignment',\n",
       " 'of',\n",
       " 'sentence',\n",
       " 'boundaries.',\n",
       " 'As',\n",
       " 'with',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'not',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'contain',\n",
       " 'punctuation',\n",
       " 'characters',\n",
       " 'which',\n",
       " 'are',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'approximating',\n",
       " 'sentence',\n",
       " 'boundaries.',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Main',\n",
       " 'articles',\n",
       " ':',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'Document',\n",
       " 'classification',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'two',\n",
       " 'main',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'topic',\n",
       " 'identiﬁcation',\n",
       " 'and',\n",
       " 'text',\n",
       " 'segmentation.',\n",
       " 'While',\n",
       " 'the',\n",
       " 'first',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'classification',\n",
       " 'of',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'text',\n",
       " ',',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'case',\n",
       " 'implies',\n",
       " 'that',\n",
       " 'a',\n",
       " 'document',\n",
       " 'may',\n",
       " 'contain',\n",
       " 'multiple',\n",
       " 'topics',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'computerized',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'these',\n",
       " 'topics',\n",
       " 'automatically',\n",
       " 'and',\n",
       " 'segment',\n",
       " 'the',\n",
       " 'text',\n",
       " 'accordingly.',\n",
       " 'The',\n",
       " 'topic',\n",
       " 'boundaries',\n",
       " 'may',\n",
       " 'be',\n",
       " 'apparent',\n",
       " 'from',\n",
       " 'section',\n",
       " 'titles',\n",
       " 'and',\n",
       " 'paragraphs.',\n",
       " 'In',\n",
       " 'other',\n",
       " 'cases',\n",
       " ',',\n",
       " 'one',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'use',\n",
       " 'techniques',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'those',\n",
       " 'used',\n",
       " 'in',\n",
       " 'document',\n",
       " 'classification.',\n",
       " 'Segmenting',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'topics',\n",
       " 'or',\n",
       " 'discourse',\n",
       " 'turns',\n",
       " 'might',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'some',\n",
       " 'natural',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'it',\n",
       " 'can',\n",
       " 'improve',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'or',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'significantly',\n",
       " '(',\n",
       " 'by',\n",
       " 'indexing/recognizing',\n",
       " 'documents',\n",
       " 'more',\n",
       " 'precisely',\n",
       " 'or',\n",
       " 'by',\n",
       " 'giving',\n",
       " 'the',\n",
       " 'specific',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'document',\n",
       " 'corresponding',\n",
       " 'to',\n",
       " 'the',\n",
       " 'query',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ')',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'also',\n",
       " 'needed',\n",
       " 'in',\n",
       " 'topic',\n",
       " 'detection',\n",
       " 'and',\n",
       " 'tracking',\n",
       " 'systems',\n",
       " 'and',\n",
       " 'text',\n",
       " 'summarizing',\n",
       " 'problems.',\n",
       " 'Many',\n",
       " 'different',\n",
       " 'approaches',\n",
       " 'have',\n",
       " 'been',\n",
       " 'tried',\n",
       " ':',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '[',\n",
       " '3',\n",
       " ']',\n",
       " 'e.g.',\n",
       " 'HMM',\n",
       " ',',\n",
       " 'lexical',\n",
       " 'chains',\n",
       " ',',\n",
       " 'passage',\n",
       " 'similarity',\n",
       " 'using',\n",
       " 'word',\n",
       " 'co-occurrence',\n",
       " ',',\n",
       " 'clustering',\n",
       " ',',\n",
       " 'topic',\n",
       " 'modeling',\n",
       " ',',\n",
       " 'etc.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'quite',\n",
       " 'an',\n",
       " 'ambiguous',\n",
       " 'task',\n",
       " '–',\n",
       " 'people',\n",
       " 'evaluating',\n",
       " 'the',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " 'systems',\n",
       " 'often',\n",
       " 'differ',\n",
       " 'in',\n",
       " 'topic',\n",
       " 'boundaries.',\n",
       " 'Hence',\n",
       " ',',\n",
       " 'text',\n",
       " 'segment',\n",
       " 'evaluation',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Same as word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      "segmentation\n",
      "-\n",
      "Wikipedia\n",
      "Text\n",
      "segmentation\n",
      "From\n",
      "Wikipedia\n",
      ",\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "Jump\n",
      "to\n",
      ":\n",
      "navigation\n",
      ",\n",
      "search\n",
      "This\n",
      "article\n",
      "needs\n",
      "additional\n",
      "citations\n",
      "for\n",
      "verification.\n",
      "Please\n",
      "help\n",
      "improve\n",
      "this\n",
      "article\n",
      "by\n",
      "adding\n",
      "citations\n",
      "to\n",
      "reliable\n",
      "sources.\n",
      "Unsourced\n",
      "material\n",
      "may\n",
      "be\n",
      "challenged\n",
      "and\n",
      "removed.\n",
      "(\n",
      "October\n",
      "2011\n",
      ")\n",
      "(\n",
      "Learn\n",
      "how\n",
      "and\n",
      "when\n",
      "to\n",
      "remove\n",
      "this\n",
      "template\n",
      "message\n",
      ")\n",
      "Text\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "dividing\n",
      "written\n",
      "text\n",
      "into\n",
      "meaningful\n",
      "units\n",
      ",\n",
      "such\n",
      "as\n",
      "words\n",
      ",\n",
      "sentences\n",
      ",\n",
      "or\n",
      "topics.\n",
      "The\n",
      "term\n",
      "applies\n",
      "both\n",
      "to\n",
      "mental\n",
      "processes\n",
      "used\n",
      "by\n",
      "humans\n",
      "when\n",
      "reading\n",
      "text\n",
      ",\n",
      "and\n",
      "to\n",
      "artificial\n",
      "processes\n",
      "implemented\n",
      "in\n",
      "computers\n",
      ",\n",
      "which\n",
      "are\n",
      "the\n",
      "subject\n",
      "of\n",
      "natural\n",
      "language\n",
      "processing.\n",
      "The\n",
      "problem\n",
      "is\n",
      "non-trivial\n",
      ",\n",
      "because\n",
      "while\n",
      "some\n",
      "written\n",
      "languages\n",
      "have\n",
      "explicit\n",
      "word\n",
      "boundary\n",
      "markers\n",
      ",\n",
      "such\n",
      "as\n",
      "the\n",
      "word\n",
      "spaces\n",
      "of\n",
      "written\n",
      "English\n",
      "and\n",
      "the\n",
      "distinctive\n",
      "initial\n",
      ",\n",
      "medial\n",
      "and\n",
      "final\n",
      "letter\n",
      "shapes\n",
      "of\n",
      "Arabic\n",
      ",\n",
      "such\n",
      "signals\n",
      "are\n",
      "sometimes\n",
      "ambiguous\n",
      "and\n",
      "not\n",
      "present\n",
      "in\n",
      "all\n",
      "written\n",
      "languages.\n",
      "Compare\n",
      "speech\n",
      "segmentation\n",
      ",\n",
      "the\n",
      "process\n",
      "of\n",
      "dividing\n",
      "speech\n",
      "into\n",
      "linguistically\n",
      "meaningful\n",
      "portions.\n",
      "Contents\n",
      "1\n",
      "Segmentation\n",
      "problems\n",
      "1.1\n",
      "Word\n",
      "segmentation\n",
      "1.2\n",
      "Intent\n",
      "segmentation\n",
      "1.3\n",
      "Sentence\n",
      "segmentation\n",
      "1.4\n",
      "Topic\n",
      "segmentation\n",
      "1.5\n",
      "Other\n",
      "segmentation\n",
      "problems\n",
      "2\n",
      "Automatic\n",
      "segmentation\n",
      "approaches\n",
      "3\n",
      "See\n",
      "also\n",
      "4\n",
      "References\n",
      "5\n",
      "External\n",
      "links\n",
      "Segmentation\n",
      "problems\n",
      "[\n",
      "edit\n",
      "]\n",
      "Word\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Word\n",
      "§\n",
      "Word\n",
      "boundaries\n",
      "Word\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "a\n",
      "string\n",
      "of\n",
      "written\n",
      "language\n",
      "into\n",
      "its\n",
      "component\n",
      "words.\n",
      "In\n",
      "English\n",
      "and\n",
      "many\n",
      "other\n",
      "languages\n",
      "using\n",
      "some\n",
      "form\n",
      "of\n",
      "the\n",
      "Latin\n",
      "alphabet\n",
      ",\n",
      "the\n",
      "space\n",
      "is\n",
      "a\n",
      "good\n",
      "approximation\n",
      "of\n",
      "a\n",
      "word\n",
      "divider\n",
      "(\n",
      "word\n",
      "delimiter\n",
      ")\n",
      ",\n",
      "although\n",
      "this\n",
      "concept\n",
      "has\n",
      "limits\n",
      "because\n",
      "of\n",
      "the\n",
      "variability\n",
      "with\n",
      "which\n",
      "languages\n",
      "emically\n",
      "regard\n",
      "collocations\n",
      "and\n",
      "compounds.\n",
      "Many\n",
      "English\n",
      "compound\n",
      "nouns\n",
      "are\n",
      "variably\n",
      "written\n",
      "(\n",
      "for\n",
      "example\n",
      ",\n",
      "ice\n",
      "box\n",
      "=\n",
      "ice-box\n",
      "=\n",
      "icebox\n",
      ";\n",
      "pig\n",
      "sty\n",
      "=\n",
      "pig-sty\n",
      "=\n",
      "pigsty\n",
      ")\n",
      "with\n",
      "a\n",
      "corresponding\n",
      "variation\n",
      "in\n",
      "whether\n",
      "speakers\n",
      "think\n",
      "of\n",
      "them\n",
      "as\n",
      "noun\n",
      "phrases\n",
      "or\n",
      "single\n",
      "nouns\n",
      ";\n",
      "there\n",
      "are\n",
      "trends\n",
      "in\n",
      "how\n",
      "norms\n",
      "are\n",
      "set\n",
      ",\n",
      "such\n",
      "as\n",
      "that\n",
      "open\n",
      "compounds\n",
      "often\n",
      "tend\n",
      "eventually\n",
      "to\n",
      "solidify\n",
      "by\n",
      "widespread\n",
      "convention\n",
      ",\n",
      "but\n",
      "variation\n",
      "remains\n",
      "systemic.\n",
      "In\n",
      "contrast\n",
      ",\n",
      "German\n",
      "compound\n",
      "nouns\n",
      "show\n",
      "less\n",
      "orthographic\n",
      "variation\n",
      ",\n",
      "with\n",
      "solidification\n",
      "being\n",
      "a\n",
      "stronger\n",
      "norm.\n",
      "However\n",
      ",\n",
      "the\n",
      "equivalent\n",
      "to\n",
      "the\n",
      "word\n",
      "space\n",
      "character\n",
      "is\n",
      "not\n",
      "found\n",
      "in\n",
      "all\n",
      "written\n",
      "scripts\n",
      ",\n",
      "and\n",
      "without\n",
      "it\n",
      "word\n",
      "segmentation\n",
      "is\n",
      "a\n",
      "difficult\n",
      "problem.\n",
      "Languages\n",
      "which\n",
      "do\n",
      "not\n",
      "have\n",
      "a\n",
      "trivial\n",
      "word\n",
      "segmentation\n",
      "process\n",
      "include\n",
      "Chinese\n",
      ",\n",
      "Japanese\n",
      ",\n",
      "where\n",
      "sentences\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited\n",
      ",\n",
      "Thai\n",
      "and\n",
      "Lao\n",
      ",\n",
      "where\n",
      "phrases\n",
      "and\n",
      "sentences\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited\n",
      ",\n",
      "and\n",
      "Vietnamese\n",
      ",\n",
      "where\n",
      "syllables\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited.\n",
      "In\n",
      "some\n",
      "writing\n",
      "systems\n",
      "however\n",
      ",\n",
      "such\n",
      "as\n",
      "the\n",
      "Ge'ez\n",
      "script\n",
      "used\n",
      "for\n",
      "Amharic\n",
      "and\n",
      "Tigrinya\n",
      "among\n",
      "other\n",
      "languages\n",
      ",\n",
      "words\n",
      "are\n",
      "explicitly\n",
      "delimited\n",
      "(\n",
      "at\n",
      "least\n",
      "historically\n",
      ")\n",
      "with\n",
      "a\n",
      "non-whitespace\n",
      "character.\n",
      "The\n",
      "Unicode\n",
      "Consortium\n",
      "has\n",
      "published\n",
      "a\n",
      "Standard\n",
      "Annex\n",
      "on\n",
      "Text\n",
      "Segmentation\n",
      ",\n",
      "[\n",
      "1\n",
      "]\n",
      "exploring\n",
      "the\n",
      "issues\n",
      "of\n",
      "segmentation\n",
      "in\n",
      "multiscript\n",
      "texts.\n",
      "Word\n",
      "splitting\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "parsing\n",
      "concatenated\n",
      "text\n",
      "(\n",
      "i.e.\n",
      "text\n",
      "that\n",
      "contains\n",
      "no\n",
      "spaces\n",
      "or\n",
      "other\n",
      "word\n",
      "separators\n",
      ")\n",
      "to\n",
      "infer\n",
      "where\n",
      "word\n",
      "breaks\n",
      "exist.\n",
      "Word\n",
      "splitting\n",
      "may\n",
      "also\n",
      "refer\n",
      "to\n",
      "the\n",
      "process\n",
      "of\n",
      "hyphenation.\n",
      "Intent\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Tri-box\n",
      "method\n",
      "Intent\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "written\n",
      "words\n",
      "into\n",
      "keyphrases\n",
      "(\n",
      "2\n",
      "or\n",
      "more\n",
      "group\n",
      "of\n",
      "words\n",
      ")\n",
      ".\n",
      "In\n",
      "English\n",
      "and\n",
      "all\n",
      "other\n",
      "languages\n",
      "the\n",
      "core\n",
      "intent\n",
      "or\n",
      "desire\n",
      "is\n",
      "identified\n",
      "and\n",
      "become\n",
      "the\n",
      "corner-stone\n",
      "of\n",
      "the\n",
      "keyphrase\n",
      "Intent\n",
      "segmentation.\n",
      "Core\n",
      "product/service\n",
      ",\n",
      "idea\n",
      ",\n",
      "action\n",
      "&\n",
      "or\n",
      "thought\n",
      "anchor\n",
      "the\n",
      "keyphrase.\n",
      "''\n",
      "[\n",
      "All\n",
      "things\n",
      "are\n",
      "made\n",
      "of\n",
      "atoms\n",
      "]\n",
      ".\n",
      "[\n",
      "Little\n",
      "particles\n",
      "that\n",
      "move\n",
      "]\n",
      "[\n",
      "around\n",
      "in\n",
      "perpetual\n",
      "motion\n",
      "]\n",
      ",\n",
      "[\n",
      "attraction\n",
      "each\n",
      "other\n",
      "]\n",
      "[\n",
      "when\n",
      "they\n",
      "are\n",
      "a\n",
      "little\n",
      "distance\n",
      "apart\n",
      "]\n",
      ",\n",
      "[\n",
      "but\n",
      "repelling\n",
      "]\n",
      "[\n",
      "upon\n",
      "being\n",
      "squeezed\n",
      "]\n",
      "[\n",
      "into\n",
      "one\n",
      "another\n",
      "]\n",
      ".\n",
      "''\n",
      "Sentence\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Sentence\n",
      "boundary\n",
      "disambiguation\n",
      "Sentence\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "a\n",
      "string\n",
      "of\n",
      "written\n",
      "language\n",
      "into\n",
      "its\n",
      "component\n",
      "sentences.\n",
      "In\n",
      "English\n",
      "and\n",
      "some\n",
      "other\n",
      "languages\n",
      ",\n",
      "using\n",
      "punctuation\n",
      ",\n",
      "particularly\n",
      "the\n",
      "full\n",
      "stop/period\n",
      "character\n",
      "is\n",
      "a\n",
      "reasonable\n",
      "approximation.\n",
      "However\n",
      "even\n",
      "in\n",
      "English\n",
      "this\n",
      "problem\n",
      "is\n",
      "not\n",
      "trivial\n",
      "due\n",
      "to\n",
      "the\n",
      "use\n",
      "of\n",
      "the\n",
      "full\n",
      "stop\n",
      "character\n",
      "for\n",
      "abbreviations\n",
      ",\n",
      "which\n",
      "may\n",
      "or\n",
      "may\n",
      "not\n",
      "also\n",
      "terminate\n",
      "a\n",
      "sentence.\n",
      "For\n",
      "example\n",
      "Mr.\n",
      "is\n",
      "not\n",
      "its\n",
      "own\n",
      "sentence\n",
      "in\n",
      "``\n",
      "Mr.\n",
      "Smith\n",
      "went\n",
      "to\n",
      "the\n",
      "shops\n",
      "in\n",
      "Jones\n",
      "Street.\n",
      "''\n",
      "When\n",
      "processing\n",
      "plain\n",
      "text\n",
      ",\n",
      "tables\n",
      "of\n",
      "abbreviations\n",
      "that\n",
      "contain\n",
      "periods\n",
      "can\n",
      "help\n",
      "prevent\n",
      "incorrect\n",
      "assignment\n",
      "of\n",
      "sentence\n",
      "boundaries.\n",
      "As\n",
      "with\n",
      "word\n",
      "segmentation\n",
      ",\n",
      "not\n",
      "all\n",
      "written\n",
      "languages\n",
      "contain\n",
      "punctuation\n",
      "characters\n",
      "which\n",
      "are\n",
      "useful\n",
      "for\n",
      "approximating\n",
      "sentence\n",
      "boundaries.\n",
      "Topic\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "Main\n",
      "articles\n",
      ":\n",
      "Topic\n",
      "analysis\n",
      "and\n",
      "Document\n",
      "classification\n",
      "Topic\n",
      "analysis\n",
      "consists\n",
      "of\n",
      "two\n",
      "main\n",
      "tasks\n",
      ":\n",
      "topic\n",
      "identiﬁcation\n",
      "and\n",
      "text\n",
      "segmentation.\n",
      "While\n",
      "the\n",
      "first\n",
      "is\n",
      "a\n",
      "simple\n",
      "classification\n",
      "of\n",
      "a\n",
      "specific\n",
      "text\n",
      ",\n",
      "the\n",
      "latter\n",
      "case\n",
      "implies\n",
      "that\n",
      "a\n",
      "document\n",
      "may\n",
      "contain\n",
      "multiple\n",
      "topics\n",
      ",\n",
      "and\n",
      "the\n",
      "task\n",
      "of\n",
      "computerized\n",
      "text\n",
      "segmentation\n",
      "may\n",
      "be\n",
      "to\n",
      "discover\n",
      "these\n",
      "topics\n",
      "automatically\n",
      "and\n",
      "segment\n",
      "the\n",
      "text\n",
      "accordingly.\n",
      "The\n",
      "topic\n",
      "boundaries\n",
      "may\n",
      "be\n",
      "apparent\n",
      "from\n",
      "section\n",
      "titles\n",
      "and\n",
      "paragraphs.\n",
      "In\n",
      "other\n",
      "cases\n",
      ",\n",
      "one\n",
      "needs\n",
      "to\n",
      "use\n",
      "techniques\n",
      "similar\n",
      "to\n",
      "those\n",
      "used\n",
      "in\n",
      "document\n",
      "classification.\n",
      "Segmenting\n",
      "the\n",
      "text\n",
      "into\n",
      "topics\n",
      "or\n",
      "discourse\n",
      "turns\n",
      "might\n",
      "be\n",
      "useful\n",
      "in\n",
      "some\n",
      "natural\n",
      "processing\n",
      "tasks\n",
      ":\n",
      "it\n",
      "can\n",
      "improve\n",
      "information\n",
      "retrieval\n",
      "or\n",
      "speech\n",
      "recognition\n",
      "significantly\n",
      "(\n",
      "by\n",
      "indexing/recognizing\n",
      "documents\n",
      "more\n",
      "precisely\n",
      "or\n",
      "by\n",
      "giving\n",
      "the\n",
      "specific\n",
      "part\n",
      "of\n",
      "a\n",
      "document\n",
      "corresponding\n",
      "to\n",
      "the\n",
      "query\n",
      "as\n",
      "a\n",
      "result\n",
      ")\n",
      ".\n",
      "It\n",
      "is\n",
      "also\n",
      "needed\n",
      "in\n",
      "topic\n",
      "detection\n",
      "and\n",
      "tracking\n",
      "systems\n",
      "and\n",
      "text\n",
      "summarizing\n",
      "problems.\n",
      "Many\n",
      "different\n",
      "approaches\n",
      "have\n",
      "been\n",
      "tried\n",
      ":\n",
      "[\n",
      "2\n",
      "]\n",
      "[\n",
      "3\n",
      "]\n",
      "e.g.\n",
      "HMM\n",
      ",\n",
      "lexical\n",
      "chains\n",
      ",\n",
      "passage\n",
      "similarity\n",
      "using\n",
      "word\n",
      "co-occurrence\n",
      ",\n",
      "clustering\n",
      ",\n",
      "topic\n",
      "modeling\n",
      ",\n",
      "etc.\n",
      "It\n",
      "is\n",
      "quite\n",
      "an\n",
      "ambiguous\n",
      "task\n",
      "–\n",
      "people\n",
      "evaluating\n",
      "the\n",
      "text\n",
      "segmentation\n",
      "systems\n",
      "often\n",
      "differ\n",
      "in\n",
      "topic\n",
      "boundaries.\n",
      "Hence\n",
      ",\n",
      "text\n",
      "segment\n",
      "evaluation\n",
      "is\n",
      "also\n",
      "a\n",
      "challenging\n",
      "problem.\n",
      "Other\n",
      "segmentation\n",
      "problems\n",
      "[\n",
      "edit\n",
      "]\n",
      "Processes\n",
      "may\n",
      "be\n",
      "required\n",
      "to\n",
      "segment\n",
      "text\n",
      "into\n",
      "segments\n",
      "besides\n",
      "mentioned\n",
      ",\n",
      "including\n",
      "morphemes\n",
      "(\n",
      "a\n",
      "task\n",
      "usually\n",
      "called\n",
      "morphological\n",
      "analysis\n",
      ")\n",
      "or\n",
      "paragraphs.\n",
      "Automatic\n",
      "segmentation\n",
      "approaches\n",
      "[\n",
      "edit\n",
      "]\n",
      "Automatic\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "in\n",
      "natural\n",
      "language\n",
      "processing\n",
      "of\n",
      "implementing\n",
      "a\n",
      "computer\n",
      "process\n",
      "to\n",
      "segment\n",
      "text.\n",
      "When\n",
      "punctuation\n",
      "and\n",
      "similar\n",
      "clues\n",
      "are\n",
      "not\n",
      "consistently\n",
      "available\n",
      ",\n",
      "the\n",
      "segmentation\n",
      "task\n",
      "often\n",
      "requires\n",
      "fairly\n",
      "non-trivial\n",
      "techniques\n",
      ",\n",
      "such\n",
      "as\n",
      "statistical\n",
      "decision-making\n",
      ",\n",
      "large\n",
      "dictionaries\n",
      ",\n",
      "as\n",
      "well\n",
      "as\n",
      "consideration\n",
      "of\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "constraints.\n",
      "Effective\n",
      "natural\n",
      "language\n",
      "processing\n",
      "systems\n",
      "and\n",
      "text\n",
      "segmentation\n",
      "tools\n",
      "usually\n",
      "operate\n",
      "on\n",
      "text\n",
      "in\n",
      "specific\n",
      "domains\n",
      "and\n",
      "sources.\n",
      "As\n",
      "an\n",
      "example\n",
      ",\n",
      "processing\n",
      "text\n",
      "used\n",
      "in\n",
      "medical\n",
      "records\n",
      "is\n",
      "a\n",
      "very\n",
      "different\n",
      "problem\n",
      "than\n",
      "processing\n",
      "news\n",
      "articles\n",
      "or\n",
      "real\n",
      "estate\n",
      "advertisements.\n",
      "The\n",
      "process\n",
      "of\n",
      "developing\n",
      "text\n",
      "segmentation\n",
      "tools\n",
      "starts\n",
      "with\n",
      "collecting\n",
      "a\n",
      "large\n",
      "corpus\n",
      "of\n",
      "text\n",
      "in\n",
      "an\n",
      "application\n",
      "domain.\n",
      "There\n",
      "are\n",
      "two\n",
      "general\n",
      "approaches\n",
      ":\n",
      "Manual\n",
      "analysis\n",
      "of\n",
      "text\n",
      "and\n",
      "writing\n",
      "custom\n",
      "software\n",
      "Annotate\n",
      "the\n",
      "sample\n",
      "corpus\n",
      "with\n",
      "boundary\n",
      "information\n",
      "and\n",
      "use\n",
      "machine\n",
      "learning\n",
      "Some\n",
      "text\n",
      "segmentation\n",
      "systems\n",
      "take\n",
      "advantage\n",
      "of\n",
      "any\n",
      "markup\n",
      "like\n",
      "HTML\n",
      "and\n",
      "know\n",
      "document\n",
      "formats\n",
      "like\n",
      "PDF\n",
      "to\n",
      "provide\n",
      "additional\n",
      "evidence\n",
      "for\n",
      "sentence\n",
      "and\n",
      "paragraph\n",
      "boundaries.\n",
      "See\n",
      "also\n",
      "[\n",
      "edit\n",
      "]\n",
      "Hyphenation\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "Speech\n",
      "segmentation\n",
      "Lexical\n",
      "analysis\n",
      "Word\n",
      "count\n",
      "Line\n",
      "breaking\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "General\n",
      "terms\n",
      "Text\n",
      "corpus\n",
      "Speech\n",
      "corpus\n",
      "Stopwords\n",
      "Bag-of-words\n",
      "AI-complete\n",
      "n-gram\n",
      "(\n",
      "Bigram\n",
      ",\n",
      "Trigram\n",
      ")\n",
      "Text\n",
      "analysis\n",
      "Text\n",
      "segmentation\n",
      "Part-of-speech\n",
      "tagging\n",
      "Text\n",
      "chunking\n",
      "Compound\n",
      "term\n",
      "processing\n",
      "Collocation\n",
      "extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "Named-entity\n",
      "recognition\n",
      "Coreference\n",
      "resolution\n",
      "Sentiment\n",
      "analysis\n",
      "Concept\n",
      "mining\n",
      "Parsing\n",
      "Word-sense\n",
      "disambiguation\n",
      "Terminology\n",
      "extraction\n",
      "Truecasing\n",
      "Automatic\n",
      "summarization\n",
      "Multi-document\n",
      "summarization\n",
      "Sentence\n",
      "extraction\n",
      "Text\n",
      "simplification\n",
      "Machine\n",
      "translation\n",
      "Computer-assisted\n",
      "Example-based\n",
      "Rule-based\n",
      "Automatic\n",
      "identification\n",
      "and\n",
      "data\n",
      "capture\n",
      "Speech\n",
      "recognition\n",
      "Speech\n",
      "synthesis\n",
      "Optical\n",
      "character\n",
      "recognition\n",
      "Natural\n",
      "language\n",
      "generation\n",
      "Topic\n",
      "model\n",
      "Pachinko\n",
      "allocation\n",
      "Latent\n",
      "Dirichlet\n",
      "allocation\n",
      "Latent\n",
      "semantic\n",
      "analysis\n",
      "Computer-assisted\n",
      "reviewing\n",
      "Automated\n",
      "essay\n",
      "scoring\n",
      "Concordancer\n",
      "Grammar\n",
      "checker\n",
      "Predictive\n",
      "text\n",
      "Spell\n",
      "checker\n",
      "Syntax\n",
      "guessing\n",
      "Natural\n",
      "language\n",
      "user\n",
      "interface\n",
      "Automated\n",
      "online\n",
      "assistant\n",
      "Chatbot\n",
      "Interactive\n",
      "fiction\n",
      "Question\n",
      "answering\n",
      "References\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "UAX\n",
      "#\n",
      "29\n",
      "^\n",
      "Freddy\n",
      "Y.\n",
      "Y.\n",
      "Choi\n",
      "(\n",
      "2000\n",
      ")\n",
      ".\n",
      "``\n",
      "Advances\n",
      "in\n",
      "domain\n",
      "independent\n",
      "linear\n",
      "text\n",
      "segmentation\n",
      "''\n",
      "(\n",
      "PDF\n",
      ")\n",
      ".\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "1st\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "North\n",
      "American\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "(\n",
      "ANLP-NAACL-00\n",
      ")\n",
      ".\n",
      "pp.\n",
      "26–33.\n",
      "^\n",
      "Jeffrey\n",
      "C.\n",
      "Reynar\n",
      "(\n",
      "1998\n",
      ")\n",
      ".\n",
      "``\n",
      "Topic\n",
      "Segmentation\n",
      ":\n",
      "Algorithms\n",
      "and\n",
      "Applications\n",
      "''\n",
      "(\n",
      "PDF\n",
      ")\n",
      ".\n",
      "IRCS-98-21.\n",
      "University\n",
      "of\n",
      "Pennsylvania.\n",
      "Retrieved\n",
      "2007-11-08.\n",
      "External\n",
      "links\n",
      "[\n",
      "edit\n",
      "]\n",
      "Word\n",
      "Segment\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Chinese.\n",
      "Word\n",
      "Split\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "designed\n",
      "to\n",
      "split\n",
      "conjoined\n",
      "words\n",
      "into\n",
      "human-readable\n",
      "text.\n",
      "Stanford\n",
      "Segmenter\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Chinese\n",
      "or\n",
      "morpheme\n",
      "segmentation\n",
      "in\n",
      "Arabic.\n",
      "KyTea\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Japanese\n",
      "and\n",
      "Chinese.\n",
      "Chinese\n",
      "Notes\n",
      "A\n",
      "Chinese–English\n",
      "dictionary\n",
      "that\n",
      "also\n",
      "does\n",
      "word\n",
      "segmentation.\n",
      "Zhihuita\n",
      "Segmentor\n",
      "A\n",
      "high\n",
      "precision\n",
      "and\n",
      "high\n",
      "performance\n",
      "Chinese\n",
      "segmentation\n",
      "freeware.\n",
      "Python\n",
      "wordsegment\n",
      "module\n",
      "An\n",
      "open\n",
      "source\n",
      "Python\n",
      "module\n",
      "for\n",
      "English\n",
      "word\n",
      "segmentation.\n",
      "Retrieved\n",
      "from\n",
      "``\n",
      "https\n",
      ":\n",
      "//en.wikipedia.org/w/index.php\n",
      "?\n",
      "title=Text_segmentation\n",
      "&\n",
      "oldid=792337667\n",
      "''\n",
      "Categories\n",
      ":\n",
      "Tasks\n",
      "of\n",
      "natural\n",
      "language\n",
      "processingHidden\n",
      "categories\n",
      ":\n",
      "Use\n",
      "dmy\n",
      "dates\n",
      "from\n",
      "March\n",
      "2016Articles\n",
      "needing\n",
      "additional\n",
      "references\n",
      "from\n",
      "October\n",
      "2011All\n",
      "articles\n",
      "needing\n",
      "additional\n",
      "references\n",
      "Navigation\n",
      "menu\n",
      "Personal\n",
      "tools\n",
      "Not\n",
      "logged\n",
      "inTalkContributionsCreate\n",
      "accountLog\n",
      "in\n",
      "Namespaces\n",
      "Article\n",
      "Talk\n",
      "Variants\n",
      "Views\n",
      "Read\n",
      "Edit\n",
      "View\n",
      "history\n",
      "More\n",
      "Search\n",
      "Navigation\n",
      "Main\n",
      "pageContentsFeatured\n",
      "contentCurrent\n",
      "eventsRandom\n",
      "articleDonate\n",
      "to\n",
      "WikipediaWikipedia\n",
      "store\n",
      "Interaction\n",
      "HelpAbout\n",
      "WikipediaCommunity\n",
      "portalRecent\n",
      "changesContact\n",
      "page\n",
      "Tools\n",
      "What\n",
      "links\n",
      "hereRelated\n",
      "changesUpload\n",
      "fileSpecial\n",
      "pagesPermanent\n",
      "linkPage\n",
      "informationWikidata\n",
      "itemCite\n",
      "this\n",
      "page\n",
      "Print/export\n",
      "Create\n",
      "a\n",
      "bookDownload\n",
      "as\n",
      "PDFPrintable\n",
      "version\n",
      "Languages\n",
      "বাংলাDeutschGalegoՀայերենBahasa\n",
      "Indonesia日本語Norsk\n",
      "nynorskPortuguêsРусский\n",
      "Edit\n",
      "links\n",
      "This\n",
      "page\n",
      "was\n",
      "last\n",
      "edited\n",
      "on\n",
      "25\n",
      "July\n",
      "2017\n",
      ",\n",
      "at\n",
      "22:51.\n",
      "Text\n",
      "is\n",
      "available\n",
      "under\n",
      "the\n",
      "Creative\n",
      "Commons\n",
      "Attribution-ShareAlike\n",
      "License\n",
      ";\n",
      "additional\n",
      "terms\n",
      "may\n",
      "apply.\n",
      "By\n",
      "using\n",
      "this\n",
      "site\n",
      ",\n",
      "you\n",
      "agree\n",
      "to\n",
      "the\n",
      "Terms\n",
      "of\n",
      "Use\n",
      "and\n",
      "Privacy\n",
      "Policy.\n",
      "Wikipedia®\n",
      "is\n",
      "a\n",
      "registered\n",
      "trademark\n",
      "of\n",
      "the\n",
      "Wikimedia\n",
      "Foundation\n",
      ",\n",
      "Inc.\n",
      ",\n",
      "a\n",
      "non-profit\n",
      "organization.\n",
      "Privacy\n",
      "policy\n",
      "About\n",
      "Wikipedia\n",
      "Disclaimers\n",
      "Contact\n",
      "Wikipedia\n",
      "Developers\n",
      "Cookie\n",
      "statement\n",
      "Mobile\n",
      "view\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.tokenize(text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Text',\n",
       " 'segmentation',\n",
       " '-',\n",
       " 'Wikipedia',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'From',\n",
       " 'Wikipedia',\n",
       " ',',\n",
       " 'the',\n",
       " 'free',\n",
       " 'encyclopedia',\n",
       " 'Jump',\n",
       " 'to',\n",
       " ':',\n",
       " 'navigation',\n",
       " ',',\n",
       " 'search',\n",
       " 'This',\n",
       " 'article',\n",
       " 'needs',\n",
       " 'additional',\n",
       " 'citations',\n",
       " 'for',\n",
       " 'verification',\n",
       " '.',\n",
       " 'Please',\n",
       " 'help',\n",
       " 'improve',\n",
       " 'this',\n",
       " 'article',\n",
       " 'by',\n",
       " 'adding',\n",
       " 'citations',\n",
       " 'to',\n",
       " 'reliable',\n",
       " 'sources',\n",
       " '.',\n",
       " 'Unsourced',\n",
       " 'material',\n",
       " 'may',\n",
       " 'be',\n",
       " 'challenged',\n",
       " 'and',\n",
       " 'removed',\n",
       " '.',\n",
       " '(',\n",
       " 'October',\n",
       " '2011',\n",
       " ')',\n",
       " '(',\n",
       " 'Learn',\n",
       " 'how',\n",
       " 'and',\n",
       " 'when',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'this',\n",
       " 'template',\n",
       " 'message',\n",
       " ')',\n",
       " 'Text',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'text',\n",
       " 'into',\n",
       " 'meaningful',\n",
       " 'units',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'words',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'or',\n",
       " 'topics',\n",
       " '.',\n",
       " 'The',\n",
       " 'term',\n",
       " 'applies',\n",
       " 'both',\n",
       " 'to',\n",
       " 'mental',\n",
       " 'processes',\n",
       " 'used',\n",
       " 'by',\n",
       " 'humans',\n",
       " 'when',\n",
       " 'reading',\n",
       " 'text',\n",
       " ',',\n",
       " 'and',\n",
       " 'to',\n",
       " 'artificial',\n",
       " 'processes',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'computers',\n",
       " ',',\n",
       " 'which',\n",
       " 'are',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'The',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'non',\n",
       " '-',\n",
       " 'trivial',\n",
       " ',',\n",
       " 'because',\n",
       " 'while',\n",
       " 'some',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'have',\n",
       " 'explicit',\n",
       " 'word',\n",
       " 'boundary',\n",
       " 'markers',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'word',\n",
       " 'spaces',\n",
       " 'of',\n",
       " 'written',\n",
       " 'English',\n",
       " 'and',\n",
       " 'the',\n",
       " 'distinctive',\n",
       " 'initial',\n",
       " ',',\n",
       " 'medial',\n",
       " 'and',\n",
       " 'final',\n",
       " 'letter',\n",
       " 'shapes',\n",
       " 'of',\n",
       " 'Arabic',\n",
       " ',',\n",
       " 'such',\n",
       " 'signals',\n",
       " 'are',\n",
       " 'sometimes',\n",
       " 'ambiguous',\n",
       " 'and',\n",
       " 'not',\n",
       " 'present',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages',\n",
       " '.',\n",
       " 'Compare',\n",
       " 'speech',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'speech',\n",
       " 'into',\n",
       " 'linguistically',\n",
       " 'meaningful',\n",
       " 'portions',\n",
       " '.',\n",
       " 'Contents',\n",
       " '1',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '1',\n",
       " '.',\n",
       " '1',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '1',\n",
       " '.',\n",
       " '2',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '1',\n",
       " '.',\n",
       " '3',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '1',\n",
       " '.',\n",
       " '4',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '1',\n",
       " '.',\n",
       " '5',\n",
       " 'Other',\n",
       " 'segmentation',\n",
       " 'problems',\n",
       " '2',\n",
       " 'Automatic',\n",
       " 'segmentation',\n",
       " 'approaches',\n",
       " '3',\n",
       " 'See',\n",
       " 'also',\n",
       " '4',\n",
       " 'References',\n",
       " '5',\n",
       " 'External',\n",
       " 'links',\n",
       " 'Segmentation',\n",
       " 'problems',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Word',\n",
       " '§',\n",
       " 'Word',\n",
       " 'boundaries',\n",
       " 'Word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'words',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'many',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'using',\n",
       " 'some',\n",
       " 'form',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Latin',\n",
       " 'alphabet',\n",
       " ',',\n",
       " 'the',\n",
       " 'space',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'approximation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'word',\n",
       " 'divider',\n",
       " '(',\n",
       " 'word',\n",
       " 'delimiter',\n",
       " '),',\n",
       " 'although',\n",
       " 'this',\n",
       " 'concept',\n",
       " 'has',\n",
       " 'limits',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'variability',\n",
       " 'with',\n",
       " 'which',\n",
       " 'languages',\n",
       " 'emically',\n",
       " 'regard',\n",
       " 'collocations',\n",
       " 'and',\n",
       " 'compounds',\n",
       " '.',\n",
       " 'Many',\n",
       " 'English',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'are',\n",
       " 'variably',\n",
       " 'written',\n",
       " '(',\n",
       " 'for',\n",
       " 'example',\n",
       " ',',\n",
       " 'ice',\n",
       " 'box',\n",
       " '=',\n",
       " 'ice',\n",
       " '-',\n",
       " 'box',\n",
       " '=',\n",
       " 'icebox',\n",
       " ';',\n",
       " 'pig',\n",
       " 'sty',\n",
       " '=',\n",
       " 'pig',\n",
       " '-',\n",
       " 'sty',\n",
       " '=',\n",
       " 'pigsty',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'corresponding',\n",
       " 'variation',\n",
       " 'in',\n",
       " 'whether',\n",
       " 'speakers',\n",
       " 'think',\n",
       " 'of',\n",
       " 'them',\n",
       " 'as',\n",
       " 'noun',\n",
       " 'phrases',\n",
       " 'or',\n",
       " 'single',\n",
       " 'nouns',\n",
       " ';',\n",
       " 'there',\n",
       " 'are',\n",
       " 'trends',\n",
       " 'in',\n",
       " 'how',\n",
       " 'norms',\n",
       " 'are',\n",
       " 'set',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'that',\n",
       " 'open',\n",
       " 'compounds',\n",
       " 'often',\n",
       " 'tend',\n",
       " 'eventually',\n",
       " 'to',\n",
       " 'solidify',\n",
       " 'by',\n",
       " 'widespread',\n",
       " 'convention',\n",
       " ',',\n",
       " 'but',\n",
       " 'variation',\n",
       " 'remains',\n",
       " 'systemic',\n",
       " '.',\n",
       " 'In',\n",
       " 'contrast',\n",
       " ',',\n",
       " 'German',\n",
       " 'compound',\n",
       " 'nouns',\n",
       " 'show',\n",
       " 'less',\n",
       " 'orthographic',\n",
       " 'variation',\n",
       " ',',\n",
       " 'with',\n",
       " 'solidification',\n",
       " 'being',\n",
       " 'a',\n",
       " 'stronger',\n",
       " 'norm',\n",
       " '.',\n",
       " 'However',\n",
       " ',',\n",
       " 'the',\n",
       " 'equivalent',\n",
       " 'to',\n",
       " 'the',\n",
       " 'word',\n",
       " 'space',\n",
       " 'character',\n",
       " 'is',\n",
       " 'not',\n",
       " 'found',\n",
       " 'in',\n",
       " 'all',\n",
       " 'written',\n",
       " 'scripts',\n",
       " ',',\n",
       " 'and',\n",
       " 'without',\n",
       " 'it',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'a',\n",
       " 'difficult',\n",
       " 'problem',\n",
       " '.',\n",
       " 'Languages',\n",
       " 'which',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'a',\n",
       " 'trivial',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " 'process',\n",
       " 'include',\n",
       " 'Chinese',\n",
       " ',',\n",
       " 'Japanese',\n",
       " ',',\n",
       " 'where',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'Thai',\n",
       " 'and',\n",
       " 'Lao',\n",
       " ',',\n",
       " 'where',\n",
       " 'phrases',\n",
       " 'and',\n",
       " 'sentences',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " ',',\n",
       " 'and',\n",
       " 'Vietnamese',\n",
       " ',',\n",
       " 'where',\n",
       " 'syllables',\n",
       " 'but',\n",
       " 'not',\n",
       " 'words',\n",
       " 'are',\n",
       " 'delimited',\n",
       " '.',\n",
       " 'In',\n",
       " 'some',\n",
       " 'writing',\n",
       " 'systems',\n",
       " 'however',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Ge',\n",
       " \"'\",\n",
       " 'ez',\n",
       " 'script',\n",
       " 'used',\n",
       " 'for',\n",
       " 'Amharic',\n",
       " 'and',\n",
       " 'Tigrinya',\n",
       " 'among',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'words',\n",
       " 'are',\n",
       " 'explicitly',\n",
       " 'delimited',\n",
       " '(',\n",
       " 'at',\n",
       " 'least',\n",
       " 'historically',\n",
       " ')',\n",
       " 'with',\n",
       " 'a',\n",
       " 'non',\n",
       " '-',\n",
       " 'whitespace',\n",
       " 'character',\n",
       " '.',\n",
       " 'The',\n",
       " 'Unicode',\n",
       " 'Consortium',\n",
       " 'has',\n",
       " 'published',\n",
       " 'a',\n",
       " 'Standard',\n",
       " 'Annex',\n",
       " 'on',\n",
       " 'Text',\n",
       " 'Segmentation',\n",
       " ',[',\n",
       " '1',\n",
       " ']',\n",
       " 'exploring',\n",
       " 'the',\n",
       " 'issues',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'in',\n",
       " 'multiscript',\n",
       " 'texts',\n",
       " '.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'parsing',\n",
       " 'concatenated',\n",
       " 'text',\n",
       " '(',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.',\n",
       " 'text',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'no',\n",
       " 'spaces',\n",
       " 'or',\n",
       " 'other',\n",
       " 'word',\n",
       " 'separators',\n",
       " ')',\n",
       " 'to',\n",
       " 'infer',\n",
       " 'where',\n",
       " 'word',\n",
       " 'breaks',\n",
       " 'exist',\n",
       " '.',\n",
       " 'Word',\n",
       " 'splitting',\n",
       " 'may',\n",
       " 'also',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'hyphenation',\n",
       " '.',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Tri',\n",
       " '-',\n",
       " 'box',\n",
       " 'method',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'written',\n",
       " 'words',\n",
       " 'into',\n",
       " 'keyphrases',\n",
       " '(',\n",
       " '2',\n",
       " 'or',\n",
       " 'more',\n",
       " 'group',\n",
       " 'of',\n",
       " 'words',\n",
       " ').',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'all',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'the',\n",
       " 'core',\n",
       " 'intent',\n",
       " 'or',\n",
       " 'desire',\n",
       " 'is',\n",
       " 'identified',\n",
       " 'and',\n",
       " 'become',\n",
       " 'the',\n",
       " 'corner',\n",
       " '-',\n",
       " 'stone',\n",
       " 'of',\n",
       " 'the',\n",
       " 'keyphrase',\n",
       " 'Intent',\n",
       " 'segmentation',\n",
       " '.',\n",
       " 'Core',\n",
       " 'product',\n",
       " '/',\n",
       " 'service',\n",
       " ',',\n",
       " 'idea',\n",
       " ',',\n",
       " 'action',\n",
       " '&',\n",
       " 'or',\n",
       " 'thought',\n",
       " 'anchor',\n",
       " 'the',\n",
       " 'keyphrase',\n",
       " '.',\n",
       " '\"[',\n",
       " 'All',\n",
       " 'things',\n",
       " 'are',\n",
       " 'made',\n",
       " 'of',\n",
       " 'atoms',\n",
       " '].',\n",
       " '[',\n",
       " 'Little',\n",
       " 'particles',\n",
       " 'that',\n",
       " 'move',\n",
       " ']',\n",
       " '[',\n",
       " 'around',\n",
       " 'in',\n",
       " 'perpetual',\n",
       " 'motion',\n",
       " '],',\n",
       " '[',\n",
       " 'attraction',\n",
       " 'each',\n",
       " 'other',\n",
       " ']',\n",
       " '[',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'a',\n",
       " 'little',\n",
       " 'distance',\n",
       " 'apart',\n",
       " '],',\n",
       " '[',\n",
       " 'but',\n",
       " 'repelling',\n",
       " ']',\n",
       " '[',\n",
       " 'upon',\n",
       " 'being',\n",
       " 'squeezed',\n",
       " ']',\n",
       " '[',\n",
       " 'into',\n",
       " 'one',\n",
       " 'another',\n",
       " '].\"',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'See',\n",
       " 'also',\n",
       " ':',\n",
       " 'Sentence',\n",
       " 'boundary',\n",
       " 'disambiguation',\n",
       " 'Sentence',\n",
       " 'segmentation',\n",
       " 'is',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'of',\n",
       " 'dividing',\n",
       " 'a',\n",
       " 'string',\n",
       " 'of',\n",
       " 'written',\n",
       " 'language',\n",
       " 'into',\n",
       " 'its',\n",
       " 'component',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'In',\n",
       " 'English',\n",
       " 'and',\n",
       " 'some',\n",
       " 'other',\n",
       " 'languages',\n",
       " ',',\n",
       " 'using',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'particularly',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " '/',\n",
       " 'period',\n",
       " 'character',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reasonable',\n",
       " 'approximation',\n",
       " '.',\n",
       " 'However',\n",
       " 'even',\n",
       " 'in',\n",
       " 'English',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'not',\n",
       " 'trivial',\n",
       " 'due',\n",
       " 'to',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " 'character',\n",
       " 'for',\n",
       " 'abbreviations',\n",
       " ',',\n",
       " 'which',\n",
       " 'may',\n",
       " 'or',\n",
       " 'may',\n",
       " 'not',\n",
       " 'also',\n",
       " 'terminate',\n",
       " 'a',\n",
       " 'sentence',\n",
       " '.',\n",
       " 'For',\n",
       " 'example',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'is',\n",
       " 'not',\n",
       " 'its',\n",
       " 'own',\n",
       " 'sentence',\n",
       " 'in',\n",
       " '\"',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Smith',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'shops',\n",
       " 'in',\n",
       " 'Jones',\n",
       " 'Street',\n",
       " '.\"',\n",
       " 'When',\n",
       " 'processing',\n",
       " 'plain',\n",
       " 'text',\n",
       " ',',\n",
       " 'tables',\n",
       " 'of',\n",
       " 'abbreviations',\n",
       " 'that',\n",
       " 'contain',\n",
       " 'periods',\n",
       " 'can',\n",
       " 'help',\n",
       " 'prevent',\n",
       " 'incorrect',\n",
       " 'assignment',\n",
       " 'of',\n",
       " 'sentence',\n",
       " 'boundaries',\n",
       " '.',\n",
       " 'As',\n",
       " 'with',\n",
       " 'word',\n",
       " 'segmentation',\n",
       " ',',\n",
       " 'not',\n",
       " 'all',\n",
       " 'written',\n",
       " 'languages',\n",
       " 'contain',\n",
       " 'punctuation',\n",
       " 'characters',\n",
       " 'which',\n",
       " 'are',\n",
       " 'useful',\n",
       " 'for',\n",
       " 'approximating',\n",
       " 'sentence',\n",
       " 'boundaries',\n",
       " '.',\n",
       " 'Topic',\n",
       " 'segmentation',\n",
       " '[',\n",
       " 'edit',\n",
       " ']',\n",
       " 'Main',\n",
       " 'articles',\n",
       " ':',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'and',\n",
       " 'Document',\n",
       " 'classification',\n",
       " 'Topic',\n",
       " 'analysis',\n",
       " 'consists',\n",
       " 'of',\n",
       " 'two',\n",
       " 'main',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'topic',\n",
       " 'identiﬁcation',\n",
       " 'and',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " '.',\n",
       " 'While',\n",
       " 'the',\n",
       " 'first',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'classification',\n",
       " 'of',\n",
       " 'a',\n",
       " 'specific',\n",
       " 'text',\n",
       " ',',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'case',\n",
       " 'implies',\n",
       " 'that',\n",
       " 'a',\n",
       " 'document',\n",
       " 'may',\n",
       " 'contain',\n",
       " 'multiple',\n",
       " 'topics',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'task',\n",
       " 'of',\n",
       " 'computerized',\n",
       " 'text',\n",
       " 'segmentation',\n",
       " 'may',\n",
       " 'be',\n",
       " 'to',\n",
       " 'discover',\n",
       " 'these',\n",
       " 'topics',\n",
       " 'automatically',\n",
       " 'and',\n",
       " 'segment',\n",
       " 'the',\n",
       " 'text',\n",
       " 'accordingly',\n",
       " '.',\n",
       " 'The',\n",
       " 'topic',\n",
       " 'boundaries',\n",
       " 'may',\n",
       " 'be',\n",
       " 'apparent',\n",
       " 'from',\n",
       " 'section',\n",
       " 'titles',\n",
       " 'and',\n",
       " 'paragraphs',\n",
       " '.',\n",
       " 'In',\n",
       " 'other',\n",
       " 'cases',\n",
       " ',',\n",
       " 'one',\n",
       " 'needs',\n",
       " 'to',\n",
       " 'use',\n",
       " 'techniques',\n",
       " 'similar',\n",
       " 'to',\n",
       " 'those',\n",
       " 'used',\n",
       " 'in',\n",
       " 'document',\n",
       " 'classification',\n",
       " '.',\n",
       " 'Segmenting',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'topics',\n",
       " 'or',\n",
       " 'discourse',\n",
       " 'turns',\n",
       " 'might',\n",
       " 'be',\n",
       " 'useful',\n",
       " 'in',\n",
       " 'some',\n",
       " 'natural',\n",
       " 'processing',\n",
       " 'tasks',\n",
       " ':',\n",
       " 'it',\n",
       " 'can',\n",
       " 'improve',\n",
       " 'information',\n",
       " 'retrieval',\n",
       " 'or',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'significantly',\n",
       " '(',\n",
       " 'by',\n",
       " 'indexing',\n",
       " '/',\n",
       " 'recognizing',\n",
       " 'documents',\n",
       " 'more',\n",
       " 'precisely',\n",
       " 'or',\n",
       " 'by',\n",
       " 'giving',\n",
       " 'the',\n",
       " 'specific',\n",
       " 'part',\n",
       " 'of',\n",
       " 'a',\n",
       " 'document',\n",
       " 'corresponding',\n",
       " 'to',\n",
       " 'the',\n",
       " 'query',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " ').',\n",
       " 'It',\n",
       " 'is',\n",
       " 'also',\n",
       " 'needed',\n",
       " 'in',\n",
       " 'topic',\n",
       " 'detection',\n",
       " 'and',\n",
       " 'tracking',\n",
       " 'systems',\n",
       " 'and',\n",
       " 'text',\n",
       " 'summarizing',\n",
       " 'problems',\n",
       " '.',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PUNCTUATION TOKENIZER\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "word_punct_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\n",
      "segmentation\n",
      "-\n",
      "Wikipedia\n",
      "Text\n",
      "segmentation\n",
      "From\n",
      "Wikipedia\n",
      ",\n",
      "the\n",
      "free\n",
      "encyclopedia\n",
      "Jump\n",
      "to\n",
      ":\n",
      "navigation\n",
      ",\n",
      "search\n",
      "This\n",
      "article\n",
      "needs\n",
      "additional\n",
      "citations\n",
      "for\n",
      "verification\n",
      ".\n",
      "Please\n",
      "help\n",
      "improve\n",
      "this\n",
      "article\n",
      "by\n",
      "adding\n",
      "citations\n",
      "to\n",
      "reliable\n",
      "sources\n",
      ".\n",
      "Unsourced\n",
      "material\n",
      "may\n",
      "be\n",
      "challenged\n",
      "and\n",
      "removed\n",
      ".\n",
      "(\n",
      "October\n",
      "2011\n",
      ")\n",
      "(\n",
      "Learn\n",
      "how\n",
      "and\n",
      "when\n",
      "to\n",
      "remove\n",
      "this\n",
      "template\n",
      "message\n",
      ")\n",
      "Text\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "dividing\n",
      "written\n",
      "text\n",
      "into\n",
      "meaningful\n",
      "units\n",
      ",\n",
      "such\n",
      "as\n",
      "words\n",
      ",\n",
      "sentences\n",
      ",\n",
      "or\n",
      "topics\n",
      ".\n",
      "The\n",
      "term\n",
      "applies\n",
      "both\n",
      "to\n",
      "mental\n",
      "processes\n",
      "used\n",
      "by\n",
      "humans\n",
      "when\n",
      "reading\n",
      "text\n",
      ",\n",
      "and\n",
      "to\n",
      "artificial\n",
      "processes\n",
      "implemented\n",
      "in\n",
      "computers\n",
      ",\n",
      "which\n",
      "are\n",
      "the\n",
      "subject\n",
      "of\n",
      "natural\n",
      "language\n",
      "processing\n",
      ".\n",
      "The\n",
      "problem\n",
      "is\n",
      "non\n",
      "-\n",
      "trivial\n",
      ",\n",
      "because\n",
      "while\n",
      "some\n",
      "written\n",
      "languages\n",
      "have\n",
      "explicit\n",
      "word\n",
      "boundary\n",
      "markers\n",
      ",\n",
      "such\n",
      "as\n",
      "the\n",
      "word\n",
      "spaces\n",
      "of\n",
      "written\n",
      "English\n",
      "and\n",
      "the\n",
      "distinctive\n",
      "initial\n",
      ",\n",
      "medial\n",
      "and\n",
      "final\n",
      "letter\n",
      "shapes\n",
      "of\n",
      "Arabic\n",
      ",\n",
      "such\n",
      "signals\n",
      "are\n",
      "sometimes\n",
      "ambiguous\n",
      "and\n",
      "not\n",
      "present\n",
      "in\n",
      "all\n",
      "written\n",
      "languages\n",
      ".\n",
      "Compare\n",
      "speech\n",
      "segmentation\n",
      ",\n",
      "the\n",
      "process\n",
      "of\n",
      "dividing\n",
      "speech\n",
      "into\n",
      "linguistically\n",
      "meaningful\n",
      "portions\n",
      ".\n",
      "Contents\n",
      "1\n",
      "Segmentation\n",
      "problems\n",
      "1\n",
      ".\n",
      "1\n",
      "Word\n",
      "segmentation\n",
      "1\n",
      ".\n",
      "2\n",
      "Intent\n",
      "segmentation\n",
      "1\n",
      ".\n",
      "3\n",
      "Sentence\n",
      "segmentation\n",
      "1\n",
      ".\n",
      "4\n",
      "Topic\n",
      "segmentation\n",
      "1\n",
      ".\n",
      "5\n",
      "Other\n",
      "segmentation\n",
      "problems\n",
      "2\n",
      "Automatic\n",
      "segmentation\n",
      "approaches\n",
      "3\n",
      "See\n",
      "also\n",
      "4\n",
      "References\n",
      "5\n",
      "External\n",
      "links\n",
      "Segmentation\n",
      "problems\n",
      "[\n",
      "edit\n",
      "]\n",
      "Word\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Word\n",
      "§\n",
      "Word\n",
      "boundaries\n",
      "Word\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "a\n",
      "string\n",
      "of\n",
      "written\n",
      "language\n",
      "into\n",
      "its\n",
      "component\n",
      "words\n",
      ".\n",
      "In\n",
      "English\n",
      "and\n",
      "many\n",
      "other\n",
      "languages\n",
      "using\n",
      "some\n",
      "form\n",
      "of\n",
      "the\n",
      "Latin\n",
      "alphabet\n",
      ",\n",
      "the\n",
      "space\n",
      "is\n",
      "a\n",
      "good\n",
      "approximation\n",
      "of\n",
      "a\n",
      "word\n",
      "divider\n",
      "(\n",
      "word\n",
      "delimiter\n",
      "),\n",
      "although\n",
      "this\n",
      "concept\n",
      "has\n",
      "limits\n",
      "because\n",
      "of\n",
      "the\n",
      "variability\n",
      "with\n",
      "which\n",
      "languages\n",
      "emically\n",
      "regard\n",
      "collocations\n",
      "and\n",
      "compounds\n",
      ".\n",
      "Many\n",
      "English\n",
      "compound\n",
      "nouns\n",
      "are\n",
      "variably\n",
      "written\n",
      "(\n",
      "for\n",
      "example\n",
      ",\n",
      "ice\n",
      "box\n",
      "=\n",
      "ice\n",
      "-\n",
      "box\n",
      "=\n",
      "icebox\n",
      ";\n",
      "pig\n",
      "sty\n",
      "=\n",
      "pig\n",
      "-\n",
      "sty\n",
      "=\n",
      "pigsty\n",
      ")\n",
      "with\n",
      "a\n",
      "corresponding\n",
      "variation\n",
      "in\n",
      "whether\n",
      "speakers\n",
      "think\n",
      "of\n",
      "them\n",
      "as\n",
      "noun\n",
      "phrases\n",
      "or\n",
      "single\n",
      "nouns\n",
      ";\n",
      "there\n",
      "are\n",
      "trends\n",
      "in\n",
      "how\n",
      "norms\n",
      "are\n",
      "set\n",
      ",\n",
      "such\n",
      "as\n",
      "that\n",
      "open\n",
      "compounds\n",
      "often\n",
      "tend\n",
      "eventually\n",
      "to\n",
      "solidify\n",
      "by\n",
      "widespread\n",
      "convention\n",
      ",\n",
      "but\n",
      "variation\n",
      "remains\n",
      "systemic\n",
      ".\n",
      "In\n",
      "contrast\n",
      ",\n",
      "German\n",
      "compound\n",
      "nouns\n",
      "show\n",
      "less\n",
      "orthographic\n",
      "variation\n",
      ",\n",
      "with\n",
      "solidification\n",
      "being\n",
      "a\n",
      "stronger\n",
      "norm\n",
      ".\n",
      "However\n",
      ",\n",
      "the\n",
      "equivalent\n",
      "to\n",
      "the\n",
      "word\n",
      "space\n",
      "character\n",
      "is\n",
      "not\n",
      "found\n",
      "in\n",
      "all\n",
      "written\n",
      "scripts\n",
      ",\n",
      "and\n",
      "without\n",
      "it\n",
      "word\n",
      "segmentation\n",
      "is\n",
      "a\n",
      "difficult\n",
      "problem\n",
      ".\n",
      "Languages\n",
      "which\n",
      "do\n",
      "not\n",
      "have\n",
      "a\n",
      "trivial\n",
      "word\n",
      "segmentation\n",
      "process\n",
      "include\n",
      "Chinese\n",
      ",\n",
      "Japanese\n",
      ",\n",
      "where\n",
      "sentences\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited\n",
      ",\n",
      "Thai\n",
      "and\n",
      "Lao\n",
      ",\n",
      "where\n",
      "phrases\n",
      "and\n",
      "sentences\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited\n",
      ",\n",
      "and\n",
      "Vietnamese\n",
      ",\n",
      "where\n",
      "syllables\n",
      "but\n",
      "not\n",
      "words\n",
      "are\n",
      "delimited\n",
      ".\n",
      "In\n",
      "some\n",
      "writing\n",
      "systems\n",
      "however\n",
      ",\n",
      "such\n",
      "as\n",
      "the\n",
      "Ge\n",
      "'\n",
      "ez\n",
      "script\n",
      "used\n",
      "for\n",
      "Amharic\n",
      "and\n",
      "Tigrinya\n",
      "among\n",
      "other\n",
      "languages\n",
      ",\n",
      "words\n",
      "are\n",
      "explicitly\n",
      "delimited\n",
      "(\n",
      "at\n",
      "least\n",
      "historically\n",
      ")\n",
      "with\n",
      "a\n",
      "non\n",
      "-\n",
      "whitespace\n",
      "character\n",
      ".\n",
      "The\n",
      "Unicode\n",
      "Consortium\n",
      "has\n",
      "published\n",
      "a\n",
      "Standard\n",
      "Annex\n",
      "on\n",
      "Text\n",
      "Segmentation\n",
      ",[\n",
      "1\n",
      "]\n",
      "exploring\n",
      "the\n",
      "issues\n",
      "of\n",
      "segmentation\n",
      "in\n",
      "multiscript\n",
      "texts\n",
      ".\n",
      "Word\n",
      "splitting\n",
      "is\n",
      "the\n",
      "process\n",
      "of\n",
      "parsing\n",
      "concatenated\n",
      "text\n",
      "(\n",
      "i\n",
      ".\n",
      "e\n",
      ".\n",
      "text\n",
      "that\n",
      "contains\n",
      "no\n",
      "spaces\n",
      "or\n",
      "other\n",
      "word\n",
      "separators\n",
      ")\n",
      "to\n",
      "infer\n",
      "where\n",
      "word\n",
      "breaks\n",
      "exist\n",
      ".\n",
      "Word\n",
      "splitting\n",
      "may\n",
      "also\n",
      "refer\n",
      "to\n",
      "the\n",
      "process\n",
      "of\n",
      "hyphenation\n",
      ".\n",
      "Intent\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Tri\n",
      "-\n",
      "box\n",
      "method\n",
      "Intent\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "written\n",
      "words\n",
      "into\n",
      "keyphrases\n",
      "(\n",
      "2\n",
      "or\n",
      "more\n",
      "group\n",
      "of\n",
      "words\n",
      ").\n",
      "In\n",
      "English\n",
      "and\n",
      "all\n",
      "other\n",
      "languages\n",
      "the\n",
      "core\n",
      "intent\n",
      "or\n",
      "desire\n",
      "is\n",
      "identified\n",
      "and\n",
      "become\n",
      "the\n",
      "corner\n",
      "-\n",
      "stone\n",
      "of\n",
      "the\n",
      "keyphrase\n",
      "Intent\n",
      "segmentation\n",
      ".\n",
      "Core\n",
      "product\n",
      "/\n",
      "service\n",
      ",\n",
      "idea\n",
      ",\n",
      "action\n",
      "&\n",
      "or\n",
      "thought\n",
      "anchor\n",
      "the\n",
      "keyphrase\n",
      ".\n",
      "\"[\n",
      "All\n",
      "things\n",
      "are\n",
      "made\n",
      "of\n",
      "atoms\n",
      "].\n",
      "[\n",
      "Little\n",
      "particles\n",
      "that\n",
      "move\n",
      "]\n",
      "[\n",
      "around\n",
      "in\n",
      "perpetual\n",
      "motion\n",
      "],\n",
      "[\n",
      "attraction\n",
      "each\n",
      "other\n",
      "]\n",
      "[\n",
      "when\n",
      "they\n",
      "are\n",
      "a\n",
      "little\n",
      "distance\n",
      "apart\n",
      "],\n",
      "[\n",
      "but\n",
      "repelling\n",
      "]\n",
      "[\n",
      "upon\n",
      "being\n",
      "squeezed\n",
      "]\n",
      "[\n",
      "into\n",
      "one\n",
      "another\n",
      "].\"\n",
      "Sentence\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "See\n",
      "also\n",
      ":\n",
      "Sentence\n",
      "boundary\n",
      "disambiguation\n",
      "Sentence\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "of\n",
      "dividing\n",
      "a\n",
      "string\n",
      "of\n",
      "written\n",
      "language\n",
      "into\n",
      "its\n",
      "component\n",
      "sentences\n",
      ".\n",
      "In\n",
      "English\n",
      "and\n",
      "some\n",
      "other\n",
      "languages\n",
      ",\n",
      "using\n",
      "punctuation\n",
      ",\n",
      "particularly\n",
      "the\n",
      "full\n",
      "stop\n",
      "/\n",
      "period\n",
      "character\n",
      "is\n",
      "a\n",
      "reasonable\n",
      "approximation\n",
      ".\n",
      "However\n",
      "even\n",
      "in\n",
      "English\n",
      "this\n",
      "problem\n",
      "is\n",
      "not\n",
      "trivial\n",
      "due\n",
      "to\n",
      "the\n",
      "use\n",
      "of\n",
      "the\n",
      "full\n",
      "stop\n",
      "character\n",
      "for\n",
      "abbreviations\n",
      ",\n",
      "which\n",
      "may\n",
      "or\n",
      "may\n",
      "not\n",
      "also\n",
      "terminate\n",
      "a\n",
      "sentence\n",
      ".\n",
      "For\n",
      "example\n",
      "Mr\n",
      ".\n",
      "is\n",
      "not\n",
      "its\n",
      "own\n",
      "sentence\n",
      "in\n",
      "\"\n",
      "Mr\n",
      ".\n",
      "Smith\n",
      "went\n",
      "to\n",
      "the\n",
      "shops\n",
      "in\n",
      "Jones\n",
      "Street\n",
      ".\"\n",
      "When\n",
      "processing\n",
      "plain\n",
      "text\n",
      ",\n",
      "tables\n",
      "of\n",
      "abbreviations\n",
      "that\n",
      "contain\n",
      "periods\n",
      "can\n",
      "help\n",
      "prevent\n",
      "incorrect\n",
      "assignment\n",
      "of\n",
      "sentence\n",
      "boundaries\n",
      ".\n",
      "As\n",
      "with\n",
      "word\n",
      "segmentation\n",
      ",\n",
      "not\n",
      "all\n",
      "written\n",
      "languages\n",
      "contain\n",
      "punctuation\n",
      "characters\n",
      "which\n",
      "are\n",
      "useful\n",
      "for\n",
      "approximating\n",
      "sentence\n",
      "boundaries\n",
      ".\n",
      "Topic\n",
      "segmentation\n",
      "[\n",
      "edit\n",
      "]\n",
      "Main\n",
      "articles\n",
      ":\n",
      "Topic\n",
      "analysis\n",
      "and\n",
      "Document\n",
      "classification\n",
      "Topic\n",
      "analysis\n",
      "consists\n",
      "of\n",
      "two\n",
      "main\n",
      "tasks\n",
      ":\n",
      "topic\n",
      "identiﬁcation\n",
      "and\n",
      "text\n",
      "segmentation\n",
      ".\n",
      "While\n",
      "the\n",
      "first\n",
      "is\n",
      "a\n",
      "simple\n",
      "classification\n",
      "of\n",
      "a\n",
      "specific\n",
      "text\n",
      ",\n",
      "the\n",
      "latter\n",
      "case\n",
      "implies\n",
      "that\n",
      "a\n",
      "document\n",
      "may\n",
      "contain\n",
      "multiple\n",
      "topics\n",
      ",\n",
      "and\n",
      "the\n",
      "task\n",
      "of\n",
      "computerized\n",
      "text\n",
      "segmentation\n",
      "may\n",
      "be\n",
      "to\n",
      "discover\n",
      "these\n",
      "topics\n",
      "automatically\n",
      "and\n",
      "segment\n",
      "the\n",
      "text\n",
      "accordingly\n",
      ".\n",
      "The\n",
      "topic\n",
      "boundaries\n",
      "may\n",
      "be\n",
      "apparent\n",
      "from\n",
      "section\n",
      "titles\n",
      "and\n",
      "paragraphs\n",
      ".\n",
      "In\n",
      "other\n",
      "cases\n",
      ",\n",
      "one\n",
      "needs\n",
      "to\n",
      "use\n",
      "techniques\n",
      "similar\n",
      "to\n",
      "those\n",
      "used\n",
      "in\n",
      "document\n",
      "classification\n",
      ".\n",
      "Segmenting\n",
      "the\n",
      "text\n",
      "into\n",
      "topics\n",
      "or\n",
      "discourse\n",
      "turns\n",
      "might\n",
      "be\n",
      "useful\n",
      "in\n",
      "some\n",
      "natural\n",
      "processing\n",
      "tasks\n",
      ":\n",
      "it\n",
      "can\n",
      "improve\n",
      "information\n",
      "retrieval\n",
      "or\n",
      "speech\n",
      "recognition\n",
      "significantly\n",
      "(\n",
      "by\n",
      "indexing\n",
      "/\n",
      "recognizing\n",
      "documents\n",
      "more\n",
      "precisely\n",
      "or\n",
      "by\n",
      "giving\n",
      "the\n",
      "specific\n",
      "part\n",
      "of\n",
      "a\n",
      "document\n",
      "corresponding\n",
      "to\n",
      "the\n",
      "query\n",
      "as\n",
      "a\n",
      "result\n",
      ").\n",
      "It\n",
      "is\n",
      "also\n",
      "needed\n",
      "in\n",
      "topic\n",
      "detection\n",
      "and\n",
      "tracking\n",
      "systems\n",
      "and\n",
      "text\n",
      "summarizing\n",
      "problems\n",
      ".\n",
      "Many\n",
      "different\n",
      "approaches\n",
      "have\n",
      "been\n",
      "tried\n",
      ":[\n",
      "2\n",
      "][\n",
      "3\n",
      "]\n",
      "e\n",
      ".\n",
      "g\n",
      ".\n",
      "HMM\n",
      ",\n",
      "lexical\n",
      "chains\n",
      ",\n",
      "passage\n",
      "similarity\n",
      "using\n",
      "word\n",
      "co\n",
      "-\n",
      "occurrence\n",
      ",\n",
      "clustering\n",
      ",\n",
      "topic\n",
      "modeling\n",
      ",\n",
      "etc\n",
      ".\n",
      "It\n",
      "is\n",
      "quite\n",
      "an\n",
      "ambiguous\n",
      "task\n",
      "–\n",
      "people\n",
      "evaluating\n",
      "the\n",
      "text\n",
      "segmentation\n",
      "systems\n",
      "often\n",
      "differ\n",
      "in\n",
      "topic\n",
      "boundaries\n",
      ".\n",
      "Hence\n",
      ",\n",
      "text\n",
      "segment\n",
      "evaluation\n",
      "is\n",
      "also\n",
      "a\n",
      "challenging\n",
      "problem\n",
      ".\n",
      "Other\n",
      "segmentation\n",
      "problems\n",
      "[\n",
      "edit\n",
      "]\n",
      "Processes\n",
      "may\n",
      "be\n",
      "required\n",
      "to\n",
      "segment\n",
      "text\n",
      "into\n",
      "segments\n",
      "besides\n",
      "mentioned\n",
      ",\n",
      "including\n",
      "morphemes\n",
      "(\n",
      "a\n",
      "task\n",
      "usually\n",
      "called\n",
      "morphological\n",
      "analysis\n",
      ")\n",
      "or\n",
      "paragraphs\n",
      ".\n",
      "Automatic\n",
      "segmentation\n",
      "approaches\n",
      "[\n",
      "edit\n",
      "]\n",
      "Automatic\n",
      "segmentation\n",
      "is\n",
      "the\n",
      "problem\n",
      "in\n",
      "natural\n",
      "language\n",
      "processing\n",
      "of\n",
      "implementing\n",
      "a\n",
      "computer\n",
      "process\n",
      "to\n",
      "segment\n",
      "text\n",
      ".\n",
      "When\n",
      "punctuation\n",
      "and\n",
      "similar\n",
      "clues\n",
      "are\n",
      "not\n",
      "consistently\n",
      "available\n",
      ",\n",
      "the\n",
      "segmentation\n",
      "task\n",
      "often\n",
      "requires\n",
      "fairly\n",
      "non\n",
      "-\n",
      "trivial\n",
      "techniques\n",
      ",\n",
      "such\n",
      "as\n",
      "statistical\n",
      "decision\n",
      "-\n",
      "making\n",
      ",\n",
      "large\n",
      "dictionaries\n",
      ",\n",
      "as\n",
      "well\n",
      "as\n",
      "consideration\n",
      "of\n",
      "syntactic\n",
      "and\n",
      "semantic\n",
      "constraints\n",
      ".\n",
      "Effective\n",
      "natural\n",
      "language\n",
      "processing\n",
      "systems\n",
      "and\n",
      "text\n",
      "segmentation\n",
      "tools\n",
      "usually\n",
      "operate\n",
      "on\n",
      "text\n",
      "in\n",
      "specific\n",
      "domains\n",
      "and\n",
      "sources\n",
      ".\n",
      "As\n",
      "an\n",
      "example\n",
      ",\n",
      "processing\n",
      "text\n",
      "used\n",
      "in\n",
      "medical\n",
      "records\n",
      "is\n",
      "a\n",
      "very\n",
      "different\n",
      "problem\n",
      "than\n",
      "processing\n",
      "news\n",
      "articles\n",
      "or\n",
      "real\n",
      "estate\n",
      "advertisements\n",
      ".\n",
      "The\n",
      "process\n",
      "of\n",
      "developing\n",
      "text\n",
      "segmentation\n",
      "tools\n",
      "starts\n",
      "with\n",
      "collecting\n",
      "a\n",
      "large\n",
      "corpus\n",
      "of\n",
      "text\n",
      "in\n",
      "an\n",
      "application\n",
      "domain\n",
      ".\n",
      "There\n",
      "are\n",
      "two\n",
      "general\n",
      "approaches\n",
      ":\n",
      "Manual\n",
      "analysis\n",
      "of\n",
      "text\n",
      "and\n",
      "writing\n",
      "custom\n",
      "software\n",
      "Annotate\n",
      "the\n",
      "sample\n",
      "corpus\n",
      "with\n",
      "boundary\n",
      "information\n",
      "and\n",
      "use\n",
      "machine\n",
      "learning\n",
      "Some\n",
      "text\n",
      "segmentation\n",
      "systems\n",
      "take\n",
      "advantage\n",
      "of\n",
      "any\n",
      "markup\n",
      "like\n",
      "HTML\n",
      "and\n",
      "know\n",
      "document\n",
      "formats\n",
      "like\n",
      "PDF\n",
      "to\n",
      "provide\n",
      "additional\n",
      "evidence\n",
      "for\n",
      "sentence\n",
      "and\n",
      "paragraph\n",
      "boundaries\n",
      ".\n",
      "See\n",
      "also\n",
      "[\n",
      "edit\n",
      "]\n",
      "Hyphenation\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "Speech\n",
      "segmentation\n",
      "Lexical\n",
      "analysis\n",
      "Word\n",
      "count\n",
      "Line\n",
      "breaking\n",
      "v\n",
      "t\n",
      "e\n",
      "Natural\n",
      "language\n",
      "processing\n",
      "General\n",
      "terms\n",
      "Text\n",
      "corpus\n",
      "Speech\n",
      "corpus\n",
      "Stopwords\n",
      "Bag\n",
      "-\n",
      "of\n",
      "-\n",
      "words\n",
      "AI\n",
      "-\n",
      "complete\n",
      "n\n",
      "-\n",
      "gram\n",
      "(\n",
      "Bigram\n",
      ",\n",
      "Trigram\n",
      ")\n",
      "Text\n",
      "analysis\n",
      "Text\n",
      "segmentation\n",
      "Part\n",
      "-\n",
      "of\n",
      "-\n",
      "speech\n",
      "tagging\n",
      "Text\n",
      "chunking\n",
      "Compound\n",
      "term\n",
      "processing\n",
      "Collocation\n",
      "extraction\n",
      "Stemming\n",
      "Lemmatisation\n",
      "Named\n",
      "-\n",
      "entity\n",
      "recognition\n",
      "Coreference\n",
      "resolution\n",
      "Sentiment\n",
      "analysis\n",
      "Concept\n",
      "mining\n",
      "Parsing\n",
      "Word\n",
      "-\n",
      "sense\n",
      "disambiguation\n",
      "Terminology\n",
      "extraction\n",
      "Truecasing\n",
      "Automatic\n",
      "summarization\n",
      "Multi\n",
      "-\n",
      "document\n",
      "summarization\n",
      "Sentence\n",
      "extraction\n",
      "Text\n",
      "simplification\n",
      "Machine\n",
      "translation\n",
      "Computer\n",
      "-\n",
      "assisted\n",
      "Example\n",
      "-\n",
      "based\n",
      "Rule\n",
      "-\n",
      "based\n",
      "Automatic\n",
      "identification\n",
      "and\n",
      "data\n",
      "capture\n",
      "Speech\n",
      "recognition\n",
      "Speech\n",
      "synthesis\n",
      "Optical\n",
      "character\n",
      "recognition\n",
      "Natural\n",
      "language\n",
      "generation\n",
      "Topic\n",
      "model\n",
      "Pachinko\n",
      "allocation\n",
      "Latent\n",
      "Dirichlet\n",
      "allocation\n",
      "Latent\n",
      "semantic\n",
      "analysis\n",
      "Computer\n",
      "-\n",
      "assisted\n",
      "reviewing\n",
      "Automated\n",
      "essay\n",
      "scoring\n",
      "Concordancer\n",
      "Grammar\n",
      "checker\n",
      "Predictive\n",
      "text\n",
      "Spell\n",
      "checker\n",
      "Syntax\n",
      "guessing\n",
      "Natural\n",
      "language\n",
      "user\n",
      "interface\n",
      "Automated\n",
      "online\n",
      "assistant\n",
      "Chatbot\n",
      "Interactive\n",
      "fiction\n",
      "Question\n",
      "answering\n",
      "References\n",
      "[\n",
      "edit\n",
      "]\n",
      "^\n",
      "UAX\n",
      "#\n",
      "29\n",
      "^\n",
      "Freddy\n",
      "Y\n",
      ".\n",
      "Y\n",
      ".\n",
      "Choi\n",
      "(\n",
      "2000\n",
      ").\n",
      "\"\n",
      "Advances\n",
      "in\n",
      "domain\n",
      "independent\n",
      "linear\n",
      "text\n",
      "segmentation\n",
      "\"\n",
      "(\n",
      "PDF\n",
      ").\n",
      "Proceedings\n",
      "of\n",
      "the\n",
      "1st\n",
      "Meeting\n",
      "of\n",
      "the\n",
      "North\n",
      "American\n",
      "Chapter\n",
      "of\n",
      "the\n",
      "Association\n",
      "for\n",
      "Computational\n",
      "Linguistics\n",
      "(\n",
      "ANLP\n",
      "-\n",
      "NAACL\n",
      "-\n",
      "00\n",
      ").\n",
      "pp\n",
      ".\n",
      "26\n",
      "–\n",
      "33\n",
      ".\n",
      "^\n",
      "Jeffrey\n",
      "C\n",
      ".\n",
      "Reynar\n",
      "(\n",
      "1998\n",
      ").\n",
      "\"\n",
      "Topic\n",
      "Segmentation\n",
      ":\n",
      "Algorithms\n",
      "and\n",
      "Applications\n",
      "\"\n",
      "(\n",
      "PDF\n",
      ").\n",
      "IRCS\n",
      "-\n",
      "98\n",
      "-\n",
      "21\n",
      ".\n",
      "University\n",
      "of\n",
      "Pennsylvania\n",
      ".\n",
      "Retrieved\n",
      "2007\n",
      "-\n",
      "11\n",
      "-\n",
      "08\n",
      ".\n",
      "External\n",
      "links\n",
      "[\n",
      "edit\n",
      "]\n",
      "Word\n",
      "Segment\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Chinese\n",
      ".\n",
      "Word\n",
      "Split\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "designed\n",
      "to\n",
      "split\n",
      "conjoined\n",
      "words\n",
      "into\n",
      "human\n",
      "-\n",
      "readable\n",
      "text\n",
      ".\n",
      "Stanford\n",
      "Segmenter\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Chinese\n",
      "or\n",
      "morpheme\n",
      "segmentation\n",
      "in\n",
      "Arabic\n",
      ".\n",
      "KyTea\n",
      "An\n",
      "open\n",
      "source\n",
      "software\n",
      "tool\n",
      "for\n",
      "word\n",
      "segmentation\n",
      "in\n",
      "Japanese\n",
      "and\n",
      "Chinese\n",
      ".\n",
      "Chinese\n",
      "Notes\n",
      "A\n",
      "Chinese\n",
      "–\n",
      "English\n",
      "dictionary\n",
      "that\n",
      "also\n",
      "does\n",
      "word\n",
      "segmentation\n",
      ".\n",
      "Zhihuita\n",
      "Segmentor\n",
      "A\n",
      "high\n",
      "precision\n",
      "and\n",
      "high\n",
      "performance\n",
      "Chinese\n",
      "segmentation\n",
      "freeware\n",
      ".\n",
      "Python\n",
      "wordsegment\n",
      "module\n",
      "An\n",
      "open\n",
      "source\n",
      "Python\n",
      "module\n",
      "for\n",
      "English\n",
      "word\n",
      "segmentation\n",
      ".\n",
      "Retrieved\n",
      "from\n",
      "\"\n",
      "https\n",
      "://\n",
      "en\n",
      ".\n",
      "wikipedia\n",
      ".\n",
      "org\n",
      "/\n",
      "w\n",
      "/\n",
      "index\n",
      ".\n",
      "php\n",
      "?\n",
      "title\n",
      "=\n",
      "Text_segmentation\n",
      "&\n",
      "oldid\n",
      "=\n",
      "792337667\n",
      "\"\n",
      "Categories\n",
      ":\n",
      "Tasks\n",
      "of\n",
      "natural\n",
      "language\n",
      "processingHidden\n",
      "categories\n",
      ":\n",
      "Use\n",
      "dmy\n",
      "dates\n",
      "from\n",
      "March\n",
      "2016Articles\n",
      "needing\n",
      "additional\n",
      "references\n",
      "from\n",
      "October\n",
      "2011All\n",
      "articles\n",
      "needing\n",
      "additional\n",
      "references\n",
      "Navigation\n",
      "menu\n",
      "Personal\n",
      "tools\n",
      "Not\n",
      "logged\n",
      "inTalkContributionsCreate\n",
      "accountLog\n",
      "in\n",
      "Namespaces\n",
      "Article\n",
      "Talk\n",
      "Variants\n",
      "Views\n",
      "Read\n",
      "Edit\n",
      "View\n",
      "history\n",
      "More\n",
      "Search\n",
      "Navigation\n",
      "Main\n",
      "pageContentsFeatured\n",
      "contentCurrent\n",
      "eventsRandom\n",
      "articleDonate\n",
      "to\n",
      "WikipediaWikipedia\n",
      "store\n",
      "Interaction\n",
      "HelpAbout\n",
      "WikipediaCommunity\n",
      "portalRecent\n",
      "changesContact\n",
      "page\n",
      "Tools\n",
      "What\n",
      "links\n",
      "hereRelated\n",
      "changesUpload\n",
      "fileSpecial\n",
      "pagesPermanent\n",
      "linkPage\n",
      "informationWikidata\n",
      "itemCite\n",
      "this\n",
      "page\n",
      "Print\n",
      "/\n",
      "export\n",
      "Create\n",
      "a\n",
      "bookDownload\n",
      "as\n",
      "PDFPrintable\n",
      "version\n",
      "Languages\n",
      "ব\n",
      "াং\n",
      "ল\n",
      "া\n",
      "DeutschGalegoՀայերենBahasa\n",
      "Indonesia日本語Norsk\n",
      "nynorskPortuguêsРусский\n",
      "Edit\n",
      "links\n",
      "This\n",
      "page\n",
      "was\n",
      "last\n",
      "edited\n",
      "on\n",
      "25\n",
      "July\n",
      "2017\n",
      ",\n",
      "at\n",
      "22\n",
      ":\n",
      "51\n",
      ".\n",
      "Text\n",
      "is\n",
      "available\n",
      "under\n",
      "the\n",
      "Creative\n",
      "Commons\n",
      "Attribution\n",
      "-\n",
      "ShareAlike\n",
      "License\n",
      ";\n",
      "additional\n",
      "terms\n",
      "may\n",
      "apply\n",
      ".\n",
      "By\n",
      "using\n",
      "this\n",
      "site\n",
      ",\n",
      "you\n",
      "agree\n",
      "to\n",
      "the\n",
      "Terms\n",
      "of\n",
      "Use\n",
      "and\n",
      "Privacy\n",
      "Policy\n",
      ".\n",
      "Wikipedia\n",
      "®\n",
      "is\n",
      "a\n",
      "registered\n",
      "trademark\n",
      "of\n",
      "the\n",
      "Wikimedia\n",
      "Foundation\n",
      ",\n",
      "Inc\n",
      ".,\n",
      "a\n",
      "non\n",
      "-\n",
      "profit\n",
      "organization\n",
      ".\n",
      "Privacy\n",
      "policy\n",
      "About\n",
      "Wikipedia\n",
      "Disclaimers\n",
      "Contact\n",
      "Wikipedia\n",
      "Developers\n",
      "Cookie\n",
      "statement\n",
      "Mobile\n",
      "view\n"
     ]
    }
   ],
   "source": [
    "for i in word_punct_tokenizer.tokenize(text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
